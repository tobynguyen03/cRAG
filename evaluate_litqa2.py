import os
import json
import pandas as pd
from string import ascii_uppercase

from aviary.env import TaskDataset

from paperqa import QueryRequest, Settings, ask
from paperqa.settings import AgentSettings
from paperqa.agents.task import TASK_DATASET_NAME
from paperqa.prompts import QA_PROMPT_TEMPLATE
from litqa2_utils import create_mcq_column

class LitQAEvaluator:
    def __init__(self, llm_config, document_path, dataset_setting):
        """
        Initialize the EvaluatorSciQAG class with necessary configurations and paths.

        Args:
            llm_config (dict): Configuration for the local LLM.
            document_path (str): Path to the directory containing documents for PaperQA.
            dataset_setting (str): The dataset being passed-- "final" (full data), "train", or "test".
        """
        self.llm_settings = self._init_llm_settings(llm_config)

        self.document_path = document_path
        self.dataset_setting = dataset_setting

        self.question_data = self._init_litqa_questions()
        self.generated_answers = []

    def _init_llm_settings(self, llm_config):
        settings = Settings(
            llm='ollama/llama3.2',
            llm_config=llm_config,
            
            summary_llm='ollama/llama3.2',
            summary_llm_config=llm_config,
            
            embedding='ollama/mxbai-embed-large',
            
            agent=AgentSettings(
                agent_llm='ollama/llama3.2', 
                agent_llm_config=local_llm_config
            ),
            use_doc_details=False,
            paper_directory="my_papers"
        )

        return settings

    def _init_litqa_questions(self):
        """Load questions and ground truth from the dataset."""
        base_query = QueryRequest(
            settings=self.llm_settings
        )
        dataset = TaskDataset.from_name(TASK_DATASET_NAME, base_query=base_query)
        formatted_questions = create_mcq_column(dataset.data)

        return formatted_questions

    def answer_questions(self):
        """Iterate through each question, use PaperQA to get an answer, and save the result."""
        for qa_data in self.question_data:
            base_question = qa_data['question']
            answer_choices = qa_data['answer_choices']
            correct_answer = qa_data['correct_answer']

            question_with_choices = QA_PROMPT_TEMPLATE.format(question=base_question, options=answer_choices)

            answer = ask(
                question_with_choices,
                settings=Settings(
                    llm='ollama/llama3.2',
                    llm_config=self.llm_config,
                    summary_llm='ollama/llama3.2',
                    summary_llm_config=self.llm_config,
                    embedding='ollama/mxbai-embed-large',
                    agent=AgentSettings(
                        agent_llm='ollama/llama3.2',
                        agent_llm_config=self.llm_config,
                    ),
                    use_doc_details=False,
                    paper_directory=self.document_path,
                ),
            )

            self.generated_answers.append({
                "question": base_question,
                "ground_truth": correct_answer,
                "system_answer": answer
            })


    def evaluate_answers(self):
        """Evaluate the answers generated by PaperQA against the ground truths with BERTScore."""
        ground_and_generated_answers = [(answer["system_answer"], answer["ground_truth"]) for answer in self.generated_answers]

        total_questions = len(ground_and_generated_answers)
        correctly_answered = 0

        for ground_truth, system_answer in ground_and_generated_answers:
            if sorted(ground_truth) == sorted(system_answer):
                correctly_answered += 1
        
        return correctly_answered / total_questions


    def save_results_to_json(self, output_path):
        """Save self.generated_answers to a JSON file in a readable format."""
        with open(output_path, 'w') as file:
            # Use indent for pretty-printing
            json.dump(self.generated_answers, file, indent=4)
        print(f"Results saved to {output_path}")

if __name__ == "__main__":
    local_llm_config = dict(
        model_list=[
            dict(
                model_name='ollama/llama3.2',
                litellm_params=dict(
                    model='ollama/llama3.2',
                    api_base="http://localhost:11434", 
                ),
            )
        ]
    )
    paper_directory = "my_papers"
    evaluator = LitQAEvaluator(local_llm_config, paper_directory, "final")

    print(evaluator.question_data[0].question)