---------------------------------------
Begin Slurm Prolog: Dec-08-2024 07:07:24
Job ID:    1030516
User ID:   lpimentel3
Account:   ece
Job name:  paperqar_multiagent_llama33_2agents_2rounds
Partition: coe-gpu
---------------------------------------
2024/12/08 07:07:25 routes.go:1189: INFO server config env="map[CUDA_VISIBLE_DEVICES:0 GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/hice1/lpimentel3/scratch/ollama_models/ OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2024-12-08T07:07:25.371-05:00 level=INFO source=images.go:755 msg="total blobs: 25"
time=2024-12-08T07:07:25.374-05:00 level=INFO source=images.go:762 msg="total unused blobs removed: 0"
time=2024-12-08T07:07:25.377-05:00 level=INFO source=routes.go:1240 msg="Listening on 127.0.0.1:11434 (version 0.4.1)"
time=2024-12-08T07:07:25.377-05:00 level=INFO source=common.go:135 msg="extracting embedded files" dir=/scratch/1030516/ollama3367164927/runners
time=2024-12-08T07:07:25.521-05:00 level=INFO source=common.go:49 msg="Dynamic LLM libraries" runners="[cpu_avx cpu_avx2 cuda_v11 cuda_v12 rocm cpu]"
time=2024-12-08T07:07:25.521-05:00 level=INFO source=gpu.go:221 msg="looking for compatible GPUs"
time=2024-12-08T07:07:26.012-05:00 level=INFO source=types.go:123 msg="inference compute" id=GPU-94395144-5534-7417-5dae-bdd2d128f6ba library=cuda variant=v12 compute=9.0 driver=12.4 name="NVIDIA H100 80GB HBM3" total="79.1 GiB" available="78.6 GiB"
[GIN] 2024/12/08 - 07:07:47 | 404 |     334.041µs |       127.0.0.1 | POST     "/api/show"
time=2024-12-08T07:07:48.100-05:00 level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-4824460d29f2058aaf6e1118a63a7a197a09bed509f0e7d4e2efb1ee273b447d gpu=GPU-94395144-5534-7417-5dae-bdd2d128f6ba parallel=4 available=84378779648 required="43.6 GiB"
time=2024-12-08T07:07:48.370-05:00 level=INFO source=server.go:105 msg="system memory" total="2015.0 GiB" free="1981.5 GiB" free_swap="7.9 GiB"
time=2024-12-08T07:07:48.371-05:00 level=INFO source=memory.go:343 msg="offload to cuda" layers.requested=-1 layers.model=81 layers.offload=81 layers.split="" memory.available="[78.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="43.6 GiB" memory.required.partial="43.6 GiB" memory.required.kv="2.5 GiB" memory.required.allocations="[43.6 GiB]" memory.weights.total="40.7 GiB" memory.weights.repeating="39.9 GiB" memory.weights.nonrepeating="822.0 MiB" memory.graph.full="1.1 GiB" memory.graph.partial="1.1 GiB"
time=2024-12-08T07:07:48.372-05:00 level=INFO source=server.go:383 msg="starting llama server" cmd="/scratch/1030516/ollama3367164927/runners/cuda_v12/ollama_llama_server --model /home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-4824460d29f2058aaf6e1118a63a7a197a09bed509f0e7d4e2efb1ee273b447d --ctx-size 8192 --batch-size 512 --n-gpu-layers 81 --threads 64 --parallel 4 --port 44773"
time=2024-12-08T07:07:48.372-05:00 level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2024-12-08T07:07:48.372-05:00 level=INFO source=server.go:562 msg="waiting for llama runner to start responding"
time=2024-12-08T07:07:48.373-05:00 level=INFO source=server.go:596 msg="waiting for server to become available" status="llm server error"
time=2024-12-08T07:07:48.944-05:00 level=INFO source=runner.go:863 msg="starting go runner"
time=2024-12-08T07:07:48.944-05:00 level=INFO source=runner.go:864 msg=system info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2024-12-08T07:07:48.944-05:00 level=INFO source=.:0 msg="Server listening on 127.0.0.1:44773"
llama_model_loader: loaded meta data with 36 key-value pairs and 724 tensors from /home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-4824460d29f2058aaf6e1118a63a7a197a09bed509f0e7d4e2efb1ee273b447d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.1 70B Instruct 2024 12
llama_model_loader: - kv   3:                            general.version str              = 2024-12
llama_model_loader: - kv   4:                           general.finetune str              = Instruct
llama_model_loader: - kv   5:                           general.basename str              = Llama-3.1
llama_model_loader: - kv   6:                         general.size_label str              = 70B
llama_model_loader: - kv   7:                            general.license str              = llama3.1
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Llama 3.1 70B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Meta Llama
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/meta-llama/Lla...
llama_model_loader: - kv  12:                               general.tags arr[str,5]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv  13:                          general.languages arr[str,7]       = ["fr", "it", "pt", "hi", "es", "th", ...
llama_model_loader: - kv  14:                          llama.block_count u32              = 80
llama_model_loader: - kv  15:                       llama.context_length u32              = 131072
llama_model_loader: - kv  16:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  17:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  18:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  19:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  20:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  21:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  22:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  23:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  24:                          general.file_type u32              = 15
llama_model_loader: - kv  25:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  26:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  27:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  28:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  29:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  30:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  31:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  32:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  33:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
time=2024-12-08T07:07:49.125-05:00 level=INFO source=server.go:596 msg="waiting for server to become available" status="llm server loading model"
llm_load_vocab: special tokens cache size = 256
llm_load_vocab: token to piece cache size = 0.7999 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 8192
llm_load_print_meta: n_layer          = 80
llm_load_print_meta: n_head           = 64
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 8
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 28672
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 70B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 70.55 B
llm_load_print_meta: model size       = 39.59 GiB (4.82 BPW) 
llm_load_print_meta: general.name     = Llama 3.1 70B Instruct 2024 12
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'
llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'
llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'
llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'
llm_load_print_meta: max token length = 256
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA H100 80GB HBM3, compute capability 9.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.68 MiB
llm_load_tensors: offloading 80 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 81/81 layers to GPU
llm_load_tensors:        CPU buffer size =   563.62 MiB
llm_load_tensors:      CUDA0 buffer size = 39979.50 MiB
llama_new_context_with_model: n_ctx      = 8192
llama_new_context_with_model: n_batch    = 2048
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2560.00 MiB
llama_new_context_with_model: KV self size  = 2560.00 MiB, K (f16): 1280.00 MiB, V (f16): 1280.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     2.08 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =  1104.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB
llama_new_context_with_model: graph nodes  = 2566
llama_new_context_with_model: graph splits = 2
time=2024-12-08T07:09:13.068-05:00 level=INFO source=server.go:601 msg="llama runner started in 84.70 seconds"
llama_model_loader: loaded meta data with 36 key-value pairs and 724 tensors from /home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-4824460d29f2058aaf6e1118a63a7a197a09bed509f0e7d4e2efb1ee273b447d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.1 70B Instruct 2024 12
llama_model_loader: - kv   3:                            general.version str              = 2024-12
llama_model_loader: - kv   4:                           general.finetune str              = Instruct
llama_model_loader: - kv   5:                           general.basename str              = Llama-3.1
llama_model_loader: - kv   6:                         general.size_label str              = 70B
llama_model_loader: - kv   7:                            general.license str              = llama3.1
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Llama 3.1 70B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Meta Llama
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/meta-llama/Lla...
llama_model_loader: - kv  12:                               general.tags arr[str,5]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv  13:                          general.languages arr[str,7]       = ["fr", "it", "pt", "hi", "es", "th", ...
llama_model_loader: - kv  14:                          llama.block_count u32              = 80
llama_model_loader: - kv  15:                       llama.context_length u32              = 131072
llama_model_loader: - kv  16:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  17:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  18:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  19:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  20:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  21:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  22:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  23:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  24:                          general.file_type u32              = 15
llama_model_loader: - kv  25:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  26:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  27:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  28:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  29:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  30:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  31:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  32:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  33:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
llm_load_vocab: special tokens cache size = 256
llm_load_vocab: token to piece cache size = 0.7999 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 70.55 B
llm_load_print_meta: model size       = 39.59 GiB (4.82 BPW) 
llm_load_print_meta: general.name     = Llama 3.1 70B Instruct 2024 12
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'
llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'
llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'
llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'
llm_load_print_meta: max token length = 256
llama_model_load: vocab only - skipping tensors
[GIN] 2024/12/08 - 07:09:16 | 200 |         1m28s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:09:16 | 404 |      199.78µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:09:16 | 404 |       138.7µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:09:17 | 200 |  1.113573432s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:09:17 | 404 |     850.956µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:09:17 | 404 |     176.336µs |       127.0.0.1 | POST     "/api/show"
time=2024-12-08T07:09:17.664-05:00 level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-94395144-5534-7417-5dae-bdd2d128f6ba library=cuda total="79.1 GiB" available="35.3 GiB"
time=2024-12-08T07:09:17.664-05:00 level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-819c2adf5ce6df2b6bd2ae4ca90d2a69f060afeb438d0c171db57daa02e39c3d gpu=GPU-94395144-5534-7417-5dae-bdd2d128f6ba parallel=1 available=37851824128 required="1.1 GiB"
time=2024-12-08T07:09:17.933-05:00 level=INFO source=server.go:105 msg="system memory" total="2015.0 GiB" free="1977.9 GiB" free_swap="7.9 GiB"
time=2024-12-08T07:09:17.933-05:00 level=INFO source=memory.go:343 msg="offload to cuda" layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[35.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.1 GiB" memory.required.partial="1.1 GiB" memory.required.kv="3.0 MiB" memory.required.allocations="[1.1 GiB]" memory.weights.total="580.2 MiB" memory.weights.repeating="520.6 MiB" memory.weights.nonrepeating="59.6 MiB" memory.graph.full="8.0 MiB" memory.graph.partial="8.0 MiB"
time=2024-12-08T07:09:17.934-05:00 level=INFO source=server.go:383 msg="starting llama server" cmd="/scratch/1030516/ollama3367164927/runners/cuda_v12/ollama_llama_server --model /home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-819c2adf5ce6df2b6bd2ae4ca90d2a69f060afeb438d0c171db57daa02e39c3d --ctx-size 512 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 45695"
time=2024-12-08T07:09:17.935-05:00 level=INFO source=sched.go:449 msg="loaded runners" count=2
time=2024-12-08T07:09:17.935-05:00 level=INFO source=server.go:562 msg="waiting for llama runner to start responding"
time=2024-12-08T07:09:17.935-05:00 level=INFO source=server.go:596 msg="waiting for server to become available" status="llm server error"
time=2024-12-08T07:09:17.972-05:00 level=INFO source=runner.go:863 msg="starting go runner"
time=2024-12-08T07:09:17.972-05:00 level=INFO source=runner.go:864 msg=system info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2024-12-08T07:09:17.973-05:00 level=INFO source=.:0 msg="Server listening on 127.0.0.1:45695"
llama_model_loader: loaded meta data with 23 key-value pairs and 389 tensors from /home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-819c2adf5ce6df2b6bd2ae4ca90d2a69f060afeb438d0c171db57daa02e39c3d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = mxbai-embed-large-v1
llama_model_loader: - kv   2:                           bert.block_count u32              = 24
llama_model_loader: - kv   3:                        bert.context_length u32              = 512
llama_model_loader: - kv   4:                      bert.embedding_length u32              = 1024
llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 4096
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 16
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                      bert.attention.causal bool             = false
llama_model_loader: - kv  10:                          bert.pooling_type u32              = 2
llama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  21:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:  243 tensors
llama_model_loader: - type  f16:  146 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 512
llm_load_print_meta: n_embd           = 1024
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 4096
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 2
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 512
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 335M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 334.09 M
llm_load_print_meta: model size       = 637.85 MiB (16.02 BPW) 
llm_load_print_meta: general.name     = mxbai-embed-large-v1
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA H100 80GB HBM3, compute capability 9.0, VMM: yes
time=2024-12-08T07:09:18.186-05:00 level=INFO source=server.go:596 msg="waiting for server to become available" status="llm server loading model"
llm_load_tensors: ggml ctx size =    0.32 MiB
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors:        CPU buffer size =    60.62 MiB
llm_load_tensors:      CUDA0 buffer size =   577.23 MiB
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =    48.00 MiB
llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    25.01 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     5.01 MiB
llama_new_context_with_model: graph nodes  = 849
llama_new_context_with_model: graph splits = 2
time=2024-12-08T07:09:28.207-05:00 level=INFO source=server.go:601 msg="llama runner started in 10.27 seconds"
llama_model_loader: loaded meta data with 23 key-value pairs and 389 tensors from /home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-819c2adf5ce6df2b6bd2ae4ca90d2a69f060afeb438d0c171db57daa02e39c3d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = mxbai-embed-large-v1
llama_model_loader: - kv   2:                           bert.block_count u32              = 24
llama_model_loader: - kv   3:                        bert.context_length u32              = 512
llama_model_loader: - kv   4:                      bert.embedding_length u32              = 1024
llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 4096
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 16
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                      bert.attention.causal bool             = false
llama_model_loader: - kv  10:                          bert.pooling_type u32              = 2
llama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  21:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:  243 tensors
llama_model_loader: - type  f16:  146 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 334.09 M
llm_load_print_meta: model size       = 637.85 MiB (16.02 BPW) 
llm_load_print_meta: general.name     = mxbai-embed-large-v1
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llama_model_load: vocab only - skipping tensors
[GIN] 2024/12/08 - 07:09:28 | 200 | 11.078444302s |       127.0.0.1 | POST     "/api/embed"
[GIN] 2024/12/08 - 07:09:28 | 404 |      89.496µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:09:28 | 404 |      71.807µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:09:28 | 404 |      30.741µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:09:44 | 200 |  16.40826629s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:09:44 | 404 |     189.235µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:09:44 | 404 |     129.853µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:09:45 | 200 |  17.27019697s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:09:45 | 404 |    1.676553ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:09:45 | 404 |     136.101µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:09:48 | 200 | 19.739286876s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:09:48 | 404 |      79.585µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:09:48 | 404 |      60.318µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:09:51 | 200 | 22.922406964s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:09:51 | 404 |     743.862µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:09:51 | 404 |     167.627µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:09:59 | 200 |  8.268070165s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:09:59 | 404 |     171.506µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:09:59 | 404 |     153.153µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:10:01 | 200 | 15.541041318s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:10:01 | 404 |     709.738µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:10:01 | 404 |      17.259µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:10:06 | 200 | 17.784343543s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:10:06 | 404 |     610.148µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:10:06 | 404 |     563.154µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:10:08 | 200 | 23.135432928s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:10:08 | 404 |       72.79µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:10:08 | 404 |      58.723µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:10:08 | 200 |  8.556899856s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:10:08 | 404 |      74.597µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:10:08 | 404 |     610.588µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:10:09 | 200 |  8.407408433s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:10:09 | 404 |     213.272µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:10:09 | 404 |     126.003µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:10:12 | 200 |   2.98751644s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:10:12 | 404 |      85.549µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:10:12 | 404 |      91.533µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:10:12 | 404 |      67.538µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:10:37 | 200 | 24.925111451s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:10:37 | 404 |     115.634µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:10:37 | 404 |     383.226µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:10:37 | 404 |      64.016µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:10:40 | 200 |  3.094881005s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:10:40 | 404 |     790.359µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:10:40 | 404 |     143.799µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:10:42 | 200 |  1.815319165s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:10:42 | 404 |     665.088µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:10:42 | 404 |     116.039µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:10:42 | 200 |   15.950361ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2024/12/08 - 07:10:42 | 404 |     134.696µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:10:42 | 404 |      44.665µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:10:42 | 404 |       36.22µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:10:57 | 200 | 14.505660403s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:10:57 | 404 |     104.174µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:10:57 | 404 |      76.132µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:10:58 | 200 | 15.810351932s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:10:58 | 404 |     182.896µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:10:58 | 404 |      89.364µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:11:05 | 200 | 22.837512204s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:11:05 | 404 |     176.686µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:11:05 | 404 |     136.518µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:11:08 | 200 | 25.359206593s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:11:08 | 404 |      85.221µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:11:08 | 404 |     727.661µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:11:13 | 200 | 15.826173896s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:11:13 | 404 |     188.171µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:11:13 | 404 |     167.187µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:11:18 | 200 | 19.731514699s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:11:18 | 404 |     183.613µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:11:18 | 404 |     170.556µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:11:18 | 200 | 10.261047778s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:11:18 | 404 |      42.046µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:11:18 | 404 |     168.235µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:11:21 | 200 |  8.515367184s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:11:21 | 404 |     185.695µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:11:21 | 404 |      16.781µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:11:23 | 200 | 18.060971083s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:11:23 | 404 |      88.151µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:11:23 | 404 |     693.922µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:11:25 | 200 |  7.560311456s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:11:25 | 404 |       185.7µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:11:25 | 404 |      81.848µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:11:28 | 200 |  2.978168163s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:11:28 | 404 |     152.336µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:11:28 | 404 |     171.254µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:11:28 | 404 |       41.11µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:11:51 | 200 | 22.403665013s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:11:51 | 404 |      365.72µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:11:51 | 404 |     479.866µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:11:51 | 404 |      71.547µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:11:54 | 200 |  3.113490379s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:11:54 | 404 |     192.229µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:11:54 | 404 |     146.028µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:11:56 | 200 |  1.834586623s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:11:56 | 404 |     206.608µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:11:56 | 404 |       80.17µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:11:56 | 200 |   15.681571ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2024/12/08 - 07:11:56 | 404 |     129.014µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:11:56 | 404 |      61.375µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:11:56 | 404 |      34.129µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:12:12 | 200 | 15.935601722s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:12:12 | 404 |     112.046µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:12:12 | 404 |     267.834µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:12:15 | 200 |  18.89492945s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:12:15 | 404 |     612.409µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:12:15 | 404 |     465.525µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:12:18 | 200 | 22.116241256s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:12:18 | 404 |     624.695µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:12:18 | 404 |     498.495µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:12:23 | 200 |  27.23730718s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:12:23 | 404 |     177.773µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:12:23 | 404 |      123.31µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:12:27 | 200 | 15.411071725s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:12:27 | 404 |     725.161µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:12:27 | 404 |      18.875µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:12:33 | 200 | 18.720774635s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:12:33 | 404 |     166.704µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:12:33 | 404 |     161.293µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:12:34 | 200 |  11.30545543s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:12:34 | 404 |      80.665µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:12:34 | 404 |     864.266µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:12:36 | 200 |  8.654946742s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:12:36 | 404 |     673.572µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:12:36 | 404 |     141.539µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:12:39 | 200 | 20.844638463s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:12:39 | 404 |     105.948µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:12:39 | 404 |     166.262µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:12:41 | 200 |  7.482310172s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:12:41 | 404 |     608.413µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:12:41 | 404 |      94.933µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:12:44 | 200 |    2.9807966s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:12:44 | 404 |     783.091µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:12:44 | 404 |      177.41µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:12:44 | 404 |      42.774µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:13:07 | 200 |   23.3884429s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:13:07 | 404 |     355.837µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:13:07 | 404 |       87.18µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:13:07 | 404 |      65.081µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:13:10 | 200 |  3.093582081s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:13:10 | 404 |     246.551µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:13:10 | 404 |     128.124µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:13:12 | 200 |  1.826295927s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:13:12 | 404 |       143.6µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:13:12 | 404 |     177.975µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:13:12 | 200 |   15.826782ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2024/12/08 - 07:13:12 | 404 |      877.54µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:13:12 | 404 |     419.061µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:13:12 | 404 |      34.958µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:13:27 | 200 | 14.783662918s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:13:27 | 404 |     765.371µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:13:27 | 404 |    1.030468ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:13:30 | 200 | 17.771224345s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:13:30 | 404 |     601.355µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:13:30 | 404 |      65.934µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:13:30 | 200 |  17.93135629s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:13:30 | 404 |      86.378µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:13:30 | 404 |     159.469µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:13:37 | 200 | 24.169143664s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:13:37 | 404 |     176.188µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:13:37 | 404 |      194.33µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:13:46 | 200 |  8.983871614s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:13:46 | 404 |     178.362µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:13:46 | 404 |     239.929µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:13:48 | 200 | 18.171412203s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:13:48 | 404 |     183.547µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:13:48 | 404 |      168.02µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:13:51 | 200 | 20.787461222s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:13:51 | 200 | 23.776511593s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:13:51 | 404 |     187.337µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:13:51 | 404 |      16.003µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:13:51 | 404 |      27.424µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:13:51 | 404 |      85.336µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:13:54 | 200 |  8.080035936s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:13:54 | 404 |     668.411µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:13:54 | 404 |     128.214µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:13:55 | 200 |  6.804431215s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:13:55 | 404 |     705.042µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:13:55 | 404 |      82.729µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:13:58 | 200 |  3.045698659s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:13:58 | 404 |     188.421µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:13:58 | 404 |      77.513µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:13:58 | 404 |      37.357µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:14:22 | 200 | 23.427749928s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:14:22 | 404 |         451µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:14:22 | 404 |     303.455µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:14:22 | 404 |      71.159µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:14:33 | 200 | 11.188770531s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:14:33 | 404 |     169.974µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:14:33 | 404 |      142.88µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:14:33 | 404 |      59.749µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:14:36 | 200 |  3.357631908s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:14:36 | 404 |     670.887µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:14:36 | 404 |      56.084µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:14:39 | 200 |  2.178375107s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:14:39 | 404 |     103.016µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:14:39 | 404 |      54.271µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:14:39 | 200 |   87.808289ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2024/12/08 - 07:14:39 | 404 |     131.223µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:14:39 | 404 |      44.166µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:14:39 | 404 |      66.851µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:14:52 | 200 | 13.301316143s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:14:52 | 404 |     173.265µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:14:52 | 404 |     214.371µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:14:54 | 200 | 15.440435021s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:14:54 | 404 |     171.868µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:14:54 | 404 |      61.282µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:14:57 | 200 | 18.657982138s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:14:57 | 404 |      742.07µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:14:57 | 404 |      18.822µs |       127.0.0.1 | POST     "/api/show"
time=2024-12-08T07:14:58.075-05:00 level=ERROR source=runner.go:426 msg="failed to decode batch" error="could not find a KV slot for the batch - try reducing the size of the batch or increase the context. code: 1"
time=2024-12-08T07:14:58.102-05:00 level=ERROR source=runner.go:426 msg="failed to decode batch" error="could not find a KV slot for the batch - try reducing the size of the batch or increase the context. code: 1"
[GIN] 2024/12/08 - 07:14:59 | 200 | 19.962601858s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:14:59 | 404 |     691.495µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:14:59 | 404 |      18.165µs |       127.0.0.1 | POST     "/api/show"
Encountered exception during tool call for tool gather_evidence: ValueError('Failed to parse JSON from text \'{\\n  "summary": "The correlations among detected metabolites differed significantly between diabetic and control rats. In diabetic rats, the correlation of glucose with urea increased dramatically, while the correlation of glucose with lactate decreased. The correlation of citrate with alanine, lactate, and succinate also decreased, whereas the correlation of citrate with pyruvate, fumarate, and urea increased. Several metabolites showed high positive correlations with each other in both diabetes and controls, but overall, the diabetic rats showed a decrease in metabolic correlation compared to control rats.",\\n  "relevance_score": and corrected to": 9\\n}\'. Your model may not be capable of supporting JSON output or our parsing technique could use some work. Try a different model or specify `Settings(prompts={\'use_json\': False})`')
[GIN] 2024/12/08 - 07:15:03 | 200 |  4.644770818s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:15:03 | 404 |     185.158µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:15:03 | 404 |      59.182µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:15:03 | 200 |   16.755164ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2024/12/08 - 07:15:03 | 404 |     515.031µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:15:03 | 404 |      20.205µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:15:08 | 200 | 14.308839611s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:15:08 | 404 |     181.648µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:15:08 | 404 |     184.402µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:15:09 | 200 | 11.172129565s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:15:09 | 404 |      69.136µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:15:09 | 404 |     896.337µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:15:11 | 200 | 19.126766147s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:15:11 | 404 |     196.318µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:15:11 | 404 |      65.553µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:15:23 | 200 | 19.428328682s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:15:23 | 404 |     361.268µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:15:23 | 404 |     522.551µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:15:23 | 200 | 19.584628589s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:15:23 | 404 |      61.599µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:15:23 | 404 |     121.928µs |       127.0.0.1 | POST     "/api/show"
time=2024-12-08T07:15:24.265-05:00 level=ERROR source=runner.go:426 msg="failed to decode batch" error="could not find a KV slot for the batch - try reducing the size of the batch or increase the context. code: 1"
[GIN] 2024/12/08 - 07:15:26 | 200 | 22.567360142s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:15:26 | 404 |      91.194µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:15:26 | 404 |     746.695µs |       127.0.0.1 | POST     "/api/show"
Encountered exception during tool call for tool gen_answer: 1 validation error for Context
score
  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='9 9', input_type=str]
    For further information visit https://errors.pydantic.dev/2.9/v/int_parsing
[GIN] 2024/12/08 - 07:15:26 | 404 |      58.549µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:15:28 | 200 | 29.014782827s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:15:35 | 200 | 26.947976075s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:15:39 | 200 | 29.961266052s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:15:41 | 200 | 17.850281047s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:15:45 | 200 | 19.318256538s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:15:45 | 404 |      100.22µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:15:45 | 404 |       30.39µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:15:48 | 200 | 44.209294926s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:15:48 | 200 |  3.092981589s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:15:48 | 404 |     191.354µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:15:48 | 404 |      17.478µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:15:48 | 200 |   16.038342ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2024/12/08 - 07:15:48 | 404 |     133.181µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:15:48 | 404 |      35.698µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:15:48 | 404 |      65.486µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:15:52 | 200 | 28.681932234s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:15:55 | 200 | 28.980465251s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:16:03 | 200 | 14.990376033s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:16:03 | 404 |     233.187µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:16:03 | 404 |     170.416µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:16:08 | 200 |  19.06007052s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:16:08 | 404 |     185.122µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:16:08 | 404 |      18.069µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:16:10 | 200 | 21.940367063s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:16:10 | 404 |     204.709µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:16:10 | 404 |     146.082µs |       127.0.0.1 | POST     "/api/show"
time=2024-12-08T07:16:11.897-05:00 level=ERROR source=runner.go:426 msg="failed to decode batch" error="could not find a KV slot for the batch - try reducing the size of the batch or increase the context. code: 1"
llama_get_logits_ith: invalid logits id 513, reason: batch.logits[513] != true
SIGSEGV: segmentation violation
PC=0x555555a5ec68 m=4 sigcode=1 addr=0x0
signal arrived during cgo execution

goroutine 7 gp=0xc0000f6000 m=4 mp=0xc000069808 [syscall]:
runtime.cgocall(0x5555558d6cc0, 0xc0003f3c78)
	runtime/cgocall.go:157 +0x4b fp=0xc0003f3c50 sp=0xc0003f3c18 pc=0x5555556593cb
github.com/ollama/ollama/llama._Cfunc_gpt_sampler_csample(0x15548802b940, 0x155490007470, 0x201)
	_cgo_gotypes.go:463 +0x4f fp=0xc0003f3c78 sp=0xc0003f3c50 pc=0x5555557564af
main.(*Server).processBatch.(*SamplingContext).Sample.func2(0xc0003fe500?, 0x2?, 0x201)
	github.com/ollama/ollama/llama/llama.go:663 +0x86 fp=0xc0003f3cc8 sp=0xc0003f3c78 pc=0x5555558d2ca6
github.com/ollama/ollama/llama.(*SamplingContext).Sample(...)
	github.com/ollama/ollama/llama/llama.go:663
main.(*Server).processBatch(0xc0000be120, 0xc000270000, 0xc0003f3f10)
	github.com/ollama/ollama/llama/runner/runner.go:458 +0x4ea fp=0xc0003f3ed0 sp=0xc0003f3cc8 pc=0x5555558d1fca
main.(*Server).run(0xc0000be120, {0x555555c10a40, 0xc000098050})
	github.com/ollama/ollama/llama/runner/runner.go:338 +0x1a5 fp=0xc0003f3fb8 sp=0xc0003f3ed0 pc=0x5555558d1765
main.main.gowrap2()
	github.com/ollama/ollama/llama/runner/runner.go:901 +0x28 fp=0xc0003f3fe0 sp=0xc0003f3fb8 pc=0x5555558d5ec8
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc0003f3fe8 sp=0xc0003f3fe0 pc=0x5555556c1de1
created by main.main in goroutine 1
	github.com/ollama/ollama/llama/runner/runner.go:901 +0xc2b

goroutine 1 gp=0xc0000061c0 m=nil [IO wait]:
runtime.gopark(0xc000050008?, 0x0?, 0xc0?, 0x61?, 0xc0003b18c0?)
	runtime/proc.go:402 +0xce fp=0xc0003b1888 sp=0xc0003b1868 pc=0x55555569000e
runtime.netpollblock(0xc0003b1920?, 0x55658b26?, 0x55?)
	runtime/netpoll.go:573 +0xf7 fp=0xc0003b18c0 sp=0xc0003b1888 pc=0x555555688257
internal/poll.runtime_pollWait(0x1554e4e19820, 0x72)
	runtime/netpoll.go:345 +0x85 fp=0xc0003b18e0 sp=0xc0003b18c0 pc=0x5555556bcaa5
internal/poll.(*pollDesc).wait(0x3?, 0x1?, 0x0)
	internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc0003b1908 sp=0xc0003b18e0 pc=0x55555570c9c7
internal/poll.(*pollDesc).waitRead(...)
	internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Accept(0xc0000f0080)
	internal/poll/fd_unix.go:611 +0x2ac fp=0xc0003b19b0 sp=0xc0003b1908 pc=0x55555570de8c
net.(*netFD).accept(0xc0000f0080)
	net/fd_unix.go:172 +0x29 fp=0xc0003b1a68 sp=0xc0003b19b0 pc=0x55555577c8a9
net.(*TCPListener).accept(0xc00004e200)
	net/tcpsock_posix.go:159 +0x1e fp=0xc0003b1a90 sp=0xc0003b1a68 pc=0x55555578d5de
net.(*TCPListener).Accept(0xc00004e200)
	net/tcpsock.go:327 +0x30 fp=0xc0003b1ac0 sp=0xc0003b1a90 pc=0x55555578c930
net/http.(*onceCloseListener).Accept(0xc0002ba360?)
	<autogenerated>:1 +0x24 fp=0xc0003b1ad8 sp=0xc0003b1ac0 pc=0x5555558b3a44
net/http.(*Server).Serve(0xc0000161e0, {0x555555c10400, 0xc00004e200})
	net/http/server.go:3260 +0x33e fp=0xc0003b1c08 sp=0xc0003b1ad8 pc=0x5555558aa85e
main.main()
	github.com/ollama/ollama/llama/runner/runner.go:921 +0xfcc fp=0xc0003b1f50 sp=0xc0003b1c08 pc=0x5555558d5c4c
runtime.main()
	runtime/proc.go:271 +0x29d fp=0xc0003b1fe0 sp=0xc0003b1f50 pc=0x55555568fbdd
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc0003b1fe8 sp=0xc0003b1fe0 pc=0x5555556c1de1

goroutine 2 gp=0xc000006c40 m=nil [force gc (idle), 6 minutes]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:402 +0xce fp=0xc000062fa8 sp=0xc000062f88 pc=0x55555569000e
runtime.goparkunlock(...)
	runtime/proc.go:408
runtime.forcegchelper()
	runtime/proc.go:326 +0xb8 fp=0xc000062fe0 sp=0xc000062fa8 pc=0x55555568fe98
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc000062fe8 sp=0xc000062fe0 pc=0x5555556c1de1
created by runtime.init.6 in goroutine 1
	runtime/proc.go:314 +0x1a

goroutine 3 gp=0xc000007180 m=nil [GC sweep wait]:
runtime.gopark(0x1?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:402 +0xce fp=0xc000063780 sp=0xc000063760 pc=0x55555569000e
runtime.goparkunlock(...)
	runtime/proc.go:408
runtime.bgsweep(0xc00007a000)
	runtime/mgcsweep.go:318 +0xdf fp=0xc0000637c8 sp=0xc000063780 pc=0x55555567ab9f
runtime.gcenable.gowrap1()
	runtime/mgc.go:203 +0x25 fp=0xc0000637e0 sp=0xc0000637c8 pc=0x55555566f685
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc0000637e8 sp=0xc0000637e0 pc=0x5555556c1de1
created by runtime.gcenable in goroutine 1
	runtime/mgc.go:203 +0x66

goroutine 4 gp=0xc000007340 m=nil [GC scavenge wait]:
runtime.gopark(0x122485a2?, 0x121ced3f?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:402 +0xce fp=0xc000063f78 sp=0xc000063f58 pc=0x55555569000e
runtime.goparkunlock(...)
	runtime/proc.go:408
runtime.(*scavengerState).park(0x555555dde4c0)
	runtime/mgcscavenge.go:425 +0x49 fp=0xc000063fa8 sp=0xc000063f78 pc=0x555555678549
runtime.bgscavenge(0xc00007a000)
	runtime/mgcscavenge.go:658 +0x59 fp=0xc000063fc8 sp=0xc000063fa8 pc=0x555555678af9
runtime.gcenable.gowrap2()
	runtime/mgc.go:204 +0x25 fp=0xc000063fe0 sp=0xc000063fc8 pc=0x55555566f625
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc000063fe8 sp=0xc000063fe0 pc=0x5555556c1de1
created by runtime.gcenable in goroutine 1
	runtime/mgc.go:204 +0xa5

goroutine 5 gp=0xc000007c00 m=nil [finalizer wait]:
runtime.gopark(0x0?, 0x555555c0c1a0?, 0x0?, 0x0?, 0x1000000010?)
	runtime/proc.go:402 +0xce fp=0xc000062620 sp=0xc000062600 pc=0x55555569000e
runtime.runfinq()
	runtime/mfinal.go:194 +0x107 fp=0xc0000627e0 sp=0xc000062620 pc=0x55555566e6c7
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc0000627e8 sp=0xc0000627e0 pc=0x5555556c1de1
created by runtime.createfing in goroutine 1
	runtime/mfinal.go:164 +0x3d

goroutine 534 gp=0xc000007dc0 m=nil [select]:
runtime.gopark(0xc0003aba80?, 0x2?, 0x18?, 0xb7?, 0xc0003ab824?)
	runtime/proc.go:402 +0xce fp=0xc0003ab698 sp=0xc0003ab678 pc=0x55555569000e
runtime.selectgo(0xc0003aba80, 0xc0003ab820, 0xc0003a5d00?, 0x0, 0x1?, 0x1)
	runtime/select.go:327 +0x725 fp=0xc0003ab7b8 sp=0xc0003ab698 pc=0x5555556a13e5
main.(*Server).completion(0xc0000be120, {0x555555c105b0, 0xc00029a380}, 0xc000292480)
	github.com/ollama/ollama/llama/runner/runner.go:652 +0x8fe fp=0xc0003abab8 sp=0xc0003ab7b8 pc=0x5555558d36de
main.(*Server).completion-fm({0x555555c105b0?, 0xc00029a380?}, 0x5555558aeb8d?)
	<autogenerated>:1 +0x36 fp=0xc0003abae8 sp=0xc0003abab8 pc=0x5555558d66b6
net/http.HandlerFunc.ServeHTTP(0xc0000a8dd0?, {0x555555c105b0?, 0xc00029a380?}, 0x10?)
	net/http/server.go:2171 +0x29 fp=0xc0003abb10 sp=0xc0003abae8 pc=0x5555558a7629
net/http.(*ServeMux).ServeHTTP(0x555555662f85?, {0x555555c105b0, 0xc00029a380}, 0xc000292480)
	net/http/server.go:2688 +0x1ad fp=0xc0003abb60 sp=0xc0003abb10 pc=0x5555558a94ad
net/http.serverHandler.ServeHTTP({0x555555c0f900?}, {0x555555c105b0?, 0xc00029a380?}, 0x6?)
	net/http/server.go:3142 +0x8e fp=0xc0003abb90 sp=0xc0003abb60 pc=0x5555558aa4ce
net/http.(*conn).serve(0xc0002ba090, {0x555555c10a08, 0xc0000a6db0})
	net/http/server.go:2044 +0x5e8 fp=0xc0003abfb8 sp=0xc0003abb90 pc=0x5555558a6268
net/http.(*Server).Serve.gowrap3()
	net/http/server.go:3290 +0x28 fp=0xc0003abfe0 sp=0xc0003abfb8 pc=0x5555558aac48
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc0003abfe8 sp=0xc0003abfe0 pc=0x5555556c1de1
created by net/http.(*Server).Serve in goroutine 1
	net/http/server.go:3290 +0x4b4

goroutine 603 gp=0xc0000f61c0 m=nil [IO wait]:
runtime.gopark(0x89?, 0xc0000e5958?, 0x40?, 0x59?, 0xb?)
	runtime/proc.go:402 +0xce fp=0xc0000e5910 sp=0xc0000e58f0 pc=0x55555569000e
runtime.netpollblock(0x5555556f6558?, 0x55658b26?, 0x55?)
	runtime/netpoll.go:573 +0xf7 fp=0xc0000e5948 sp=0xc0000e5910 pc=0x555555688257
internal/poll.runtime_pollWait(0x1554e4e19538, 0x72)
	runtime/netpoll.go:345 +0x85 fp=0xc0000e5968 sp=0xc0000e5948 pc=0x5555556bcaa5
internal/poll.(*pollDesc).wait(0xc0000f1700?, 0xc0003a2000?, 0x0)
	internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc0000e5990 sp=0xc0000e5968 pc=0x55555570c9c7
internal/poll.(*pollDesc).waitRead(...)
	internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Read(0xc0000f1700, {0xc0003a2000, 0x1000, 0x1000})
	internal/poll/fd_unix.go:164 +0x27a fp=0xc0000e5a28 sp=0xc0000e5990 pc=0x55555570d51a
net.(*netFD).Read(0xc0000f1700, {0xc0003a2000?, 0xc0000e5a98?, 0x55555570ce85?})
	net/fd_posix.go:55 +0x25 fp=0xc0000e5a70 sp=0xc0000e5a28 pc=0x55555577b7a5
net.(*conn).Read(0xc00022a030, {0xc0003a2000?, 0x0?, 0xc0001fc368?})
	net/net.go:185 +0x45 fp=0xc0000e5ab8 sp=0xc0000e5a70 pc=0x555555785a65
net.(*TCPConn).Read(0xc0001fc360?, {0xc0003a2000?, 0xc0000f1700?, 0xc0000e5af0?})
	<autogenerated>:1 +0x25 fp=0xc0000e5ae8 sp=0xc0000e5ab8 pc=0x555555791445
net/http.(*connReader).Read(0xc0001fc360, {0xc0003a2000, 0x1000, 0x1000})
	net/http/server.go:789 +0x14b fp=0xc0000e5b38 sp=0xc0000e5ae8 pc=0x5555558a066b
bufio.(*Reader).fill(0xc000190660)
	bufio/bufio.go:110 +0x103 fp=0xc0000e5b70 sp=0xc0000e5b38 pc=0x55555585cf63
bufio.(*Reader).Peek(0xc000190660, 0x4)
	bufio/bufio.go:148 +0x53 fp=0xc0000e5b90 sp=0xc0000e5b70 pc=0x55555585d093
net/http.(*conn).serve(0xc0002ba360, {0x555555c10a08, 0xc0000a6db0})
	net/http/server.go:2079 +0x749 fp=0xc0000e5fb8 sp=0xc0000e5b90 pc=0x5555558a63c9
net/http.(*Server).Serve.gowrap3()
	net/http/server.go:3290 +0x28 fp=0xc0000e5fe0 sp=0xc0000e5fb8 pc=0x5555558aac48
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc0000e5fe8 sp=0xc0000e5fe0 pc=0x5555556c1de1
created by net/http.(*Server).Serve in goroutine 1
	net/http/server.go:3290 +0x4b4

goroutine 372 gp=0xc000224540 m=nil [GC worker (idle), 2 minutes]:
runtime.gopark(0x1b086a3706b55b?, 0xc0002b06d0?, 0x0?, 0x0?, 0x555555ec7060?)
	runtime/proc.go:402 +0xce fp=0xc00005ff50 sp=0xc00005ff30 pc=0x55555569000e
runtime.gcBgMarkWorker()
	runtime/mgc.go:1310 +0xe5 fp=0xc00005ffe0 sp=0xc00005ff50 pc=0x555555671585
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc00005ffe8 sp=0xc00005ffe0 pc=0x5555556c1de1
created by runtime.gcBgMarkStartWorkers in goroutine 358
	runtime/mgc.go:1234 +0x1c

goroutine 618 gp=0xc0000f68c0 m=nil [IO wait]:
runtime.gopark(0x10?, 0x10?, 0xf0?, 0x2d?, 0xb?)
	runtime/proc.go:402 +0xce fp=0xc0002a2da8 sp=0xc0002a2d88 pc=0x55555569000e
runtime.netpollblock(0x5555556f6558?, 0x55658b26?, 0x55?)
	runtime/netpoll.go:573 +0xf7 fp=0xc0002a2de0 sp=0xc0002a2da8 pc=0x555555688257
internal/poll.runtime_pollWait(0x1554e4e19630, 0x72)
	runtime/netpoll.go:345 +0x85 fp=0xc0002a2e00 sp=0xc0002a2de0 pc=0x5555556bcaa5
internal/poll.(*pollDesc).wait(0xc00038c000?, 0xc00010e191?, 0x0)
	internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc0002a2e28 sp=0xc0002a2e00 pc=0x55555570c9c7
internal/poll.(*pollDesc).waitRead(...)
	internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Read(0xc00038c000, {0xc00010e191, 0x1, 0x1})
	internal/poll/fd_unix.go:164 +0x27a fp=0xc0002a2ec0 sp=0xc0002a2e28 pc=0x55555570d51a
net.(*netFD).Read(0xc00038c000, {0xc00010e191?, 0xc0002a2f48?, 0x5555556be6d0?})
	net/fd_posix.go:55 +0x25 fp=0xc0002a2f08 sp=0xc0002a2ec0 pc=0x55555577b7a5
net.(*conn).Read(0xc00038e000, {0xc00010e191?, 0x0?, 0x555555ec7060?})
	net/net.go:185 +0x45 fp=0xc0002a2f50 sp=0xc0002a2f08 pc=0x555555785a65
net.(*TCPConn).Read(0xc00010e180?, {0xc00010e191?, 0x0?, 0x0?})
	<autogenerated>:1 +0x25 fp=0xc0002a2f80 sp=0xc0002a2f50 pc=0x555555791445
net/http.(*connReader).backgroundRead(0xc00010e180)
	net/http/server.go:681 +0x37 fp=0xc0002a2fc8 sp=0xc0002a2f80 pc=0x5555558a01d7
net/http.(*connReader).startBackgroundRead.gowrap2()
	net/http/server.go:677 +0x25 fp=0xc0002a2fe0 sp=0xc0002a2fc8 pc=0x5555558a0105
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc0002a2fe8 sp=0xc0002a2fe0 pc=0x5555556c1de1
created by net/http.(*connReader).startBackgroundRead in goroutine 534
	net/http/server.go:677 +0xba

goroutine 392 gp=0xc000102700 m=nil [GC worker (idle)]:
runtime.gopark(0x1b0883d6ca68e2?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:402 +0xce fp=0xc00029ef50 sp=0xc00029ef30 pc=0x55555569000e
runtime.gcBgMarkWorker()
	runtime/mgc.go:1310 +0xe5 fp=0xc00029efe0 sp=0xc00029ef50 pc=0x555555671585
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc00029efe8 sp=0xc00029efe0 pc=0x5555556c1de1
created by runtime.gcBgMarkStartWorkers in goroutine 358
	runtime/mgc.go:1234 +0x1c

goroutine 393 gp=0xc0001028c0 m=nil [GC worker (idle), 1 minutes]:
runtime.gopark(0x1b0883d6cadf51?, 0x3?, 0x6e?, 0x1a?, 0x0?)
	runtime/proc.go:402 +0xce fp=0xc00029f750 sp=0xc00029f730 pc=0x55555569000e
runtime.gcBgMarkWorker()
	runtime/mgc.go:1310 +0xe5 fp=0xc00029f7e0 sp=0xc00029f750 pc=0x555555671585
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc00029f7e8 sp=0xc00029f7e0 pc=0x5555556c1de1
created by runtime.gcBgMarkStartWorkers in goroutine 358
	runtime/mgc.go:1234 +0x1c

goroutine 394 gp=0xc000102a80 m=nil [GC worker (idle)]:
runtime.gopark(0x1b0883d6cafea9?, 0x1?, 0x35?, 0x71?, 0x0?)
	runtime/proc.go:402 +0xce fp=0xc00029ff50 sp=0xc00029ff30 pc=0x55555569000e
runtime.gcBgMarkWorker()
	runtime/mgc.go:1310 +0xe5 fp=0xc00029ffe0 sp=0xc00029ff50 pc=0x555555671585
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc00029ffe8 sp=0xc00029ffe0 pc=0x5555556c1de1
created by runtime.gcBgMarkStartWorkers in goroutine 358
	runtime/mgc.go:1234 +0x1c

goroutine 589 gp=0xc000224fc0 m=nil [IO wait]:
runtime.gopark(0x10?, 0x10?, 0xf0?, 0x15?, 0xb?)
	runtime/proc.go:402 +0xce fp=0xc0002a15a8 sp=0xc0002a1588 pc=0x55555569000e
runtime.netpollblock(0x5555556f6558?, 0x55658b26?, 0x55?)
	runtime/netpoll.go:573 +0xf7 fp=0xc0002a15e0 sp=0xc0002a15a8 pc=0x555555688257
internal/poll.runtime_pollWait(0x1554e4e19440, 0x72)
	runtime/netpoll.go:345 +0x85 fp=0xc0002a1600 sp=0xc0002a15e0 pc=0x5555556bcaa5
internal/poll.(*pollDesc).wait(0xc0002c4280?, 0xc0002ac6d1?, 0x0)
	internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc0002a1628 sp=0xc0002a1600 pc=0x55555570c9c7
internal/poll.(*pollDesc).waitRead(...)
	internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Read(0xc0002c4280, {0xc0002ac6d1, 0x1, 0x1})
	internal/poll/fd_unix.go:164 +0x27a fp=0xc0002a16c0 sp=0xc0002a1628 pc=0x55555570d51a
net.(*netFD).Read(0xc0002c4280, {0xc0002ac6d1?, 0xc0002a1748?, 0x5555556be6d0?})
	net/fd_posix.go:55 +0x25 fp=0xc0002a1708 sp=0xc0002a16c0 pc=0x55555577b7a5
net.(*conn).Read(0xc00022a048, {0xc0002ac6d1?, 0x0?, 0x555555ec7060?})
	net/net.go:185 +0x45 fp=0xc0002a1750 sp=0xc0002a1708 pc=0x555555785a65
net.(*TCPConn).Read(0xc0002ac6c0?, {0xc0002ac6d1?, 0x0?, 0x0?})
	<autogenerated>:1 +0x25 fp=0xc0002a1780 sp=0xc0002a1750 pc=0x555555791445
net/http.(*connReader).backgroundRead(0xc0002ac6c0)
	net/http/server.go:681 +0x37 fp=0xc0002a17c8 sp=0xc0002a1780 pc=0x5555558a01d7
net/http.(*connReader).startBackgroundRead.gowrap2()
	net/http/server.go:677 +0x25 fp=0xc0002a17e0 sp=0xc0002a17c8 pc=0x5555558a0105
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc0002a17e8 sp=0xc0002a17e0 pc=0x5555556c1de1
created by net/http.(*connReader).startBackgroundRead in goroutine 566
	net/http/server.go:677 +0xba

goroutine 616 gp=0xc000225340 m=nil [IO wait]:
runtime.gopark(0x10?, 0x10?, 0xf0?, 0x75?, 0xb?)
	runtime/proc.go:402 +0xce fp=0xc0001b75a8 sp=0xc0001b7588 pc=0x55555569000e
runtime.netpollblock(0x5555556f6558?, 0x55658b26?, 0x55?)
	runtime/netpoll.go:573 +0xf7 fp=0xc0001b75e0 sp=0xc0001b75a8 pc=0x555555688257
internal/poll.runtime_pollWait(0x1554e4e19728, 0x72)
	runtime/netpoll.go:345 +0x85 fp=0xc0001b7600 sp=0xc0001b75e0 pc=0x5555556bcaa5
internal/poll.(*pollDesc).wait(0xc0002c4380?, 0xc0001fc461?, 0x0)
	internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc0001b7628 sp=0xc0001b7600 pc=0x55555570c9c7
internal/poll.(*pollDesc).waitRead(...)
	internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Read(0xc0002c4380, {0xc0001fc461, 0x1, 0x1})
	internal/poll/fd_unix.go:164 +0x27a fp=0xc0001b76c0 sp=0xc0001b7628 pc=0x55555570d51a
net.(*netFD).Read(0xc0002c4380, {0xc0001fc461?, 0xc0001b7748?, 0x5555556be6d0?})
	net/fd_posix.go:55 +0x25 fp=0xc0001b7708 sp=0xc0001b76c0 pc=0x55555577b7a5
net.(*conn).Read(0xc00022a058, {0xc0001fc461?, 0x0?, 0x555555ec7060?})
	net/net.go:185 +0x45 fp=0xc0001b7750 sp=0xc0001b7708 pc=0x555555785a65
net.(*TCPConn).Read(0xc0002ac6c0?, {0xc0001fc461?, 0x0?, 0x0?})
	<autogenerated>:1 +0x25 fp=0xc0001b7780 sp=0xc0001b7750 pc=0x555555791445
net/http.(*connReader).backgroundRead(0xc0001fc450)
	net/http/server.go:681 +0x37 fp=0xc0001b77c8 sp=0xc0001b7780 pc=0x5555558a01d7
net/http.(*connReader).startBackgroundRead.gowrap2()
	net/http/server.go:677 +0x25 fp=0xc0001b77e0 sp=0xc0001b77c8 pc=0x5555558a0105
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc0001b77e8 sp=0xc0001b77e0 pc=0x5555556c1de1
created by net/http.(*connReader).startBackgroundRead in goroutine 567
	net/http/server.go:677 +0xba

goroutine 606 gp=0xc000225500 m=nil [IO wait]:
runtime.gopark(0x10?, 0x10?, 0xf0?, 0x45?, 0xb?)
	runtime/proc.go:402 +0xce fp=0xc0001b45a8 sp=0xc0001b4588 pc=0x55555569000e
runtime.netpollblock(0x5555556f6558?, 0x55658b26?, 0x55?)
	runtime/netpoll.go:573 +0xf7 fp=0xc0001b45e0 sp=0xc0001b45a8 pc=0x555555688257
internal/poll.runtime_pollWait(0x1554e4e19348, 0x72)
	runtime/netpoll.go:345 +0x85 fp=0xc0001b4600 sp=0xc0001b45e0 pc=0x5555556bcaa5
internal/poll.(*pollDesc).wait(0xc000203280?, 0xc000268101?, 0x0)
	internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc0001b4628 sp=0xc0001b4600 pc=0x55555570c9c7
internal/poll.(*pollDesc).waitRead(...)
	internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Read(0xc000203280, {0xc000268101, 0x1, 0x1})
	internal/poll/fd_unix.go:164 +0x27a fp=0xc0001b46c0 sp=0xc0001b4628 pc=0x55555570d51a
net.(*netFD).Read(0xc000203280, {0xc000268101?, 0xc0001b4748?, 0x5555556be6d0?})
	net/fd_posix.go:55 +0x25 fp=0xc0001b4708 sp=0xc0001b46c0 pc=0x55555577b7a5
net.(*conn).Read(0xc00020c000, {0xc000268101?, 0x0?, 0x555555ec7060?})
	net/net.go:185 +0x45 fp=0xc0001b4750 sp=0xc0001b4708 pc=0x555555785a65
net.(*TCPConn).Read(0xc0002680f0?, {0xc000268101?, 0x0?, 0x0?})
	<autogenerated>:1 +0x25 fp=0xc0001b4780 sp=0xc0001b4750 pc=0x555555791445
net/http.(*connReader).backgroundRead(0xc0002680f0)
	net/http/server.go:681 +0x37 fp=0xc0001b47c8 sp=0xc0001b4780 pc=0x5555558a01d7
net/http.(*connReader).startBackgroundRead.gowrap2()
	net/http/server.go:677 +0x25 fp=0xc0001b47e0 sp=0xc0001b47c8 pc=0x5555558a0105
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc0001b47e8 sp=0xc0001b47e0 pc=0x5555556c1de1
created by net/http.(*connReader).startBackgroundRead in goroutine 569
	net/http/server.go:677 +0xba

goroutine 566 gp=0xc0002256c0 m=nil [select]:
runtime.gopark(0xc0003ada80?, 0x2?, 0x18?, 0xd7?, 0xc0003ad824?)
	runtime/proc.go:402 +0xce fp=0xc0003ad698 sp=0xc0003ad678 pc=0x55555569000e
runtime.selectgo(0xc0003ada80, 0xc0003ad820, 0xc0003a5d80?, 0x0, 0x1?, 0x1)
	runtime/select.go:327 +0x725 fp=0xc0003ad7b8 sp=0xc0003ad698 pc=0x5555556a13e5
main.(*Server).completion(0xc0000be120, {0x555555c105b0, 0xc0003fa460}, 0xc0002086c0)
	github.com/ollama/ollama/llama/runner/runner.go:652 +0x8fe fp=0xc0003adab8 sp=0xc0003ad7b8 pc=0x5555558d36de
main.(*Server).completion-fm({0x555555c105b0?, 0xc0003fa460?}, 0x5555558aeb8d?)
	<autogenerated>:1 +0x36 fp=0xc0003adae8 sp=0xc0003adab8 pc=0x5555558d66b6
net/http.HandlerFunc.ServeHTTP(0xc0000a8dd0?, {0x555555c105b0?, 0xc0003fa460?}, 0x10?)
	net/http/server.go:2171 +0x29 fp=0xc0003adb10 sp=0xc0003adae8 pc=0x5555558a7629
net/http.(*ServeMux).ServeHTTP(0x555555662f85?, {0x555555c105b0, 0xc0003fa460}, 0xc0002086c0)
	net/http/server.go:2688 +0x1ad fp=0xc0003adb60 sp=0xc0003adb10 pc=0x5555558a94ad
net/http.serverHandler.ServeHTTP({0x555555c0f900?}, {0x555555c105b0?, 0xc0003fa460?}, 0x6?)
	net/http/server.go:3142 +0x8e fp=0xc0003adb90 sp=0xc0003adb60 pc=0x5555558aa4ce
net/http.(*conn).serve(0xc000298480, {0x555555c10a08, 0xc0000a6db0})
	net/http/server.go:2044 +0x5e8 fp=0xc0003adfb8 sp=0xc0003adb90 pc=0x5555558a6268
net/http.(*Server).Serve.gowrap3()
	net/http/server.go:3290 +0x28 fp=0xc0003adfe0 sp=0xc0003adfb8 pc=0x5555558aac48
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc0003adfe8 sp=0xc0003adfe0 pc=0x5555556c1de1
created by net/http.(*Server).Serve in goroutine 1
	net/http/server.go:3290 +0x4b4

goroutine 567 gp=0xc000103340 m=nil [select]:
runtime.gopark(0xc0000e7a80?, 0x2?, 0x18?, 0x77?, 0xc0000e7824?)
	runtime/proc.go:402 +0xce fp=0xc0000e7698 sp=0xc0000e7678 pc=0x55555569000e
runtime.selectgo(0xc0000e7a80, 0xc0000e7820, 0xc0004c2280?, 0x0, 0x1?, 0x1)
	runtime/select.go:327 +0x725 fp=0xc0000e77b8 sp=0xc0000e7698 pc=0x5555556a13e5
main.(*Server).completion(0xc0000be120, {0x555555c105b0, 0xc00029a1c0}, 0xc000292240)
	github.com/ollama/ollama/llama/runner/runner.go:652 +0x8fe fp=0xc0000e7ab8 sp=0xc0000e77b8 pc=0x5555558d36de
main.(*Server).completion-fm({0x555555c105b0?, 0xc00029a1c0?}, 0x5555558aeb8d?)
	<autogenerated>:1 +0x36 fp=0xc0000e7ae8 sp=0xc0000e7ab8 pc=0x5555558d66b6
net/http.HandlerFunc.ServeHTTP(0xc0000a8dd0?, {0x555555c105b0?, 0xc00029a1c0?}, 0x10?)
	net/http/server.go:2171 +0x29 fp=0xc0000e7b10 sp=0xc0000e7ae8 pc=0x5555558a7629
net/http.(*ServeMux).ServeHTTP(0x555555662f85?, {0x555555c105b0, 0xc00029a1c0}, 0xc000292240)
	net/http/server.go:2688 +0x1ad fp=0xc0000e7b60 sp=0xc0000e7b10 pc=0x5555558a94ad
net/http.serverHandler.ServeHTTP({0x555555c0f900?}, {0x555555c105b0?, 0xc00029a1c0?}, 0x6?)
	net/http/server.go:3142 +0x8e fp=0xc0000e7b90 sp=0xc0000e7b60 pc=0x5555558aa4ce
net/http.(*conn).serve(0xc000298510, {0x555555c10a08, 0xc0000a6db0})
	net/http/server.go:2044 +0x5e8 fp=0xc0000e7fb8 sp=0xc0000e7b90 pc=0x5555558a6268
net/http.(*Server).Serve.gowrap3()
	net/http/server.go:3290 +0x28 fp=0xc0000e7fe0 sp=0xc0000e7fb8 pc=0x5555558aac48
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc0000e7fe8 sp=0xc0000e7fe0 pc=0x5555556c1de1
created by net/http.(*Server).Serve in goroutine 1
	net/http/server.go:3290 +0x4b4

goroutine 569 gp=0xc0001036c0 m=nil [select]:
runtime.gopark(0xc0003f7a80?, 0x2?, 0x18?, 0x77?, 0xc0003f7824?)
	runtime/proc.go:402 +0xce fp=0xc0003f7698 sp=0xc0003f7678 pc=0x55555569000e
runtime.selectgo(0xc0003f7a80, 0xc0003f7820, 0xc0004c2300?, 0x0, 0x1?, 0x1)
	runtime/select.go:327 +0x725 fp=0xc0003f77b8 sp=0xc0003f7698 pc=0x5555556a13e5
main.(*Server).completion(0xc0000be120, {0x555555c105b0, 0xc00017c540}, 0xc0000c0a20)
	github.com/ollama/ollama/llama/runner/runner.go:652 +0x8fe fp=0xc0003f7ab8 sp=0xc0003f77b8 pc=0x5555558d36de
main.(*Server).completion-fm({0x555555c105b0?, 0xc00017c540?}, 0x5555558aeb8d?)
	<autogenerated>:1 +0x36 fp=0xc0003f7ae8 sp=0xc0003f7ab8 pc=0x5555558d66b6
net/http.HandlerFunc.ServeHTTP(0xc0000a8dd0?, {0x555555c105b0?, 0xc00017c540?}, 0x10?)
	net/http/server.go:2171 +0x29 fp=0xc0003f7b10 sp=0xc0003f7ae8 pc=0x5555558a7629
net/http.(*ServeMux).ServeHTTP(0x555555662f85?, {0x555555c105b0, 0xc00017c540}, 0xc0000c0a20)
	net/http/server.go:2688 +0x1ad fp=0xc0003f7b60 sp=0xc0003f7b10 pc=0x5555558a94ad
net/http.serverHandler.ServeHTTP({0x555555c0f900?}, {0x555555c105b0?, 0xc00017c540?}, 0x6?)
	net/http/server.go:3142 +0x8e fp=0xc0003f7b90 sp=0xc0003f7b60 pc=0x5555558aa4ce
net/http.(*conn).serve(0xc0002ba000, {0x555555c10a08, 0xc0000a6db0})
	net/http/server.go:2044 +0x5e8 fp=0xc0003f7fb8 sp=0xc0003f7b90 pc=0x5555558a6268
net/http.(*Server).Serve.gowrap3()
	net/http/server.go:3290 +0x28 fp=0xc0003f7fe0 sp=0xc0003f7fb8 pc=0x5555558aac48
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc0003f7fe8 sp=0xc0003f7fe0 pc=0x5555556c1de1
created by net/http.(*Server).Serve in goroutine 1
	net/http/server.go:3290 +0x4b4

rax    0x0
rbx    0x0
rcx    0x1f500
rdx    0x155250068600
rdi    0x155250068600
rsi    0x1f500
rbp    0x155490007470
rsp    0x15549dddb7e0
r8     0x1f500
r9     0x15549dddb540
r10    0x1554e4f6d078
r11    0x8187ddd4af2e1cc3
r12    0x201
r13    0x15548802ba10
r14    0x15548802b940
r15    0x0
rip    0x555555a5ec68
rflags 0x10206
cs     0x33
fs     0x0
gs     0x0
[GIN] 2024/12/08 - 07:16:12 | 500 | 23.522175732s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:16:12 | 400 |      43.231µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:16:12 | 400 |      26.712µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:23:46 | 404 |     416.278µs |       127.0.0.1 | POST     "/api/show"
time=2024-12-08T07:23:51.821-05:00 level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=5.212915146 model=/home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-4824460d29f2058aaf6e1118a63a7a197a09bed509f0e7d4e2efb1ee273b447d
time=2024-12-08T07:23:52.077-05:00 level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=5.469541258 model=/home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-4824460d29f2058aaf6e1118a63a7a197a09bed509f0e7d4e2efb1ee273b447d
time=2024-12-08T07:23:52.395-05:00 level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-4824460d29f2058aaf6e1118a63a7a197a09bed509f0e7d4e2efb1ee273b447d gpu=GPU-94395144-5534-7417-5dae-bdd2d128f6ba parallel=4 available=84378779648 required="43.6 GiB"
time=2024-12-08T07:23:52.617-05:00 level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=6.00962363 model=/home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-4824460d29f2058aaf6e1118a63a7a197a09bed509f0e7d4e2efb1ee273b447d
time=2024-12-08T07:23:52.872-05:00 level=INFO source=server.go:105 msg="system memory" total="2015.0 GiB" free="1978.3 GiB" free_swap="7.9 GiB"
time=2024-12-08T07:23:52.872-05:00 level=INFO source=memory.go:343 msg="offload to cuda" layers.requested=-1 layers.model=81 layers.offload=81 layers.split="" memory.available="[78.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="43.6 GiB" memory.required.partial="43.6 GiB" memory.required.kv="2.5 GiB" memory.required.allocations="[43.6 GiB]" memory.weights.total="40.7 GiB" memory.weights.repeating="39.9 GiB" memory.weights.nonrepeating="822.0 MiB" memory.graph.full="1.1 GiB" memory.graph.partial="1.1 GiB"
time=2024-12-08T07:23:52.873-05:00 level=INFO source=server.go:383 msg="starting llama server" cmd="/scratch/1030516/ollama3367164927/runners/cuda_v12/ollama_llama_server --model /home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-4824460d29f2058aaf6e1118a63a7a197a09bed509f0e7d4e2efb1ee273b447d --ctx-size 8192 --batch-size 512 --n-gpu-layers 81 --threads 64 --parallel 4 --port 39721"
time=2024-12-08T07:23:52.874-05:00 level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2024-12-08T07:23:52.874-05:00 level=INFO source=server.go:562 msg="waiting for llama runner to start responding"
time=2024-12-08T07:23:52.874-05:00 level=WARN source=server.go:569 msg="client connection closed before server finished loading, aborting load"
time=2024-12-08T07:23:52.874-05:00 level=ERROR source=sched.go:455 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
[GIN] 2024/12/08 - 07:23:52 | 499 |         7m35s |       127.0.0.1 | POST     "/api/generate"
time=2024-12-08T07:23:52.919-05:00 level=INFO source=runner.go:863 msg="starting go runner"
time=2024-12-08T07:23:52.919-05:00 level=INFO source=runner.go:864 msg=system info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2024-12-08T07:23:52.920-05:00 level=INFO source=.:0 msg="Server listening on 127.0.0.1:39721"
llama_model_loader: loaded meta data with 36 key-value pairs and 724 tensors from /home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-4824460d29f2058aaf6e1118a63a7a197a09bed509f0e7d4e2efb1ee273b447d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.1 70B Instruct 2024 12
llama_model_loader: - kv   3:                            general.version str              = 2024-12
llama_model_loader: - kv   4:                           general.finetune str              = Instruct
llama_model_loader: - kv   5:                           general.basename str              = Llama-3.1
llama_model_loader: - kv   6:                         general.size_label str              = 70B
llama_model_loader: - kv   7:                            general.license str              = llama3.1
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Llama 3.1 70B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Meta Llama
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/meta-llama/Lla...
llama_model_loader: - kv  12:                               general.tags arr[str,5]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv  13:                          general.languages arr[str,7]       = ["fr", "it", "pt", "hi", "es", "th", ...
llama_model_loader: - kv  14:                          llama.block_count u32              = 80
llama_model_loader: - kv  15:                       llama.context_length u32              = 131072
llama_model_loader: - kv  16:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  17:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  18:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  19:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  20:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  21:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  22:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  23:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  24:                          general.file_type u32              = 15
llama_model_loader: - kv  25:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  26:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  27:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  28:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  29:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  30:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  31:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  32:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  33:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
time=2024-12-08T07:23:58.107-05:00 level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=5.233002538 model=/home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-4824460d29f2058aaf6e1118a63a7a197a09bed509f0e7d4e2efb1ee273b447d
time=2024-12-08T07:23:58.371-05:00 level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=5.497121693 model=/home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-4824460d29f2058aaf6e1118a63a7a197a09bed509f0e7d4e2efb1ee273b447d
time=2024-12-08T07:23:58.667-05:00 level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-4824460d29f2058aaf6e1118a63a7a197a09bed509f0e7d4e2efb1ee273b447d gpu=GPU-94395144-5534-7417-5dae-bdd2d128f6ba parallel=4 available=84378779648 required="43.6 GiB"
time=2024-12-08T07:23:58.891-05:00 level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=6.016268683 model=/home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-4824460d29f2058aaf6e1118a63a7a197a09bed509f0e7d4e2efb1ee273b447d
time=2024-12-08T07:23:59.149-05:00 level=INFO source=server.go:105 msg="system memory" total="2015.0 GiB" free="1978.2 GiB" free_swap="7.9 GiB"
time=2024-12-08T07:23:59.150-05:00 level=INFO source=memory.go:343 msg="offload to cuda" layers.requested=-1 layers.model=81 layers.offload=81 layers.split="" memory.available="[78.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="43.6 GiB" memory.required.partial="43.6 GiB" memory.required.kv="2.5 GiB" memory.required.allocations="[43.6 GiB]" memory.weights.total="40.7 GiB" memory.weights.repeating="39.9 GiB" memory.weights.nonrepeating="822.0 MiB" memory.graph.full="1.1 GiB" memory.graph.partial="1.1 GiB"
time=2024-12-08T07:23:59.151-05:00 level=INFO source=server.go:383 msg="starting llama server" cmd="/scratch/1030516/ollama3367164927/runners/cuda_v12/ollama_llama_server --model /home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-4824460d29f2058aaf6e1118a63a7a197a09bed509f0e7d4e2efb1ee273b447d --ctx-size 8192 --batch-size 512 --n-gpu-layers 81 --threads 64 --parallel 4 --port 44581"
time=2024-12-08T07:23:59.151-05:00 level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2024-12-08T07:23:59.151-05:00 level=INFO source=server.go:562 msg="waiting for llama runner to start responding"
time=2024-12-08T07:23:59.152-05:00 level=INFO source=server.go:596 msg="waiting for server to become available" status="llm server error"
time=2024-12-08T07:23:59.188-05:00 level=INFO source=runner.go:863 msg="starting go runner"
time=2024-12-08T07:23:59.189-05:00 level=INFO source=runner.go:864 msg=system info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2024-12-08T07:23:59.189-05:00 level=INFO source=.:0 msg="Server listening on 127.0.0.1:44581"
llama_model_loader: loaded meta data with 36 key-value pairs and 724 tensors from /home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-4824460d29f2058aaf6e1118a63a7a197a09bed509f0e7d4e2efb1ee273b447d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.1 70B Instruct 2024 12
llama_model_loader: - kv   3:                            general.version str              = 2024-12
llama_model_loader: - kv   4:                           general.finetune str              = Instruct
llama_model_loader: - kv   5:                           general.basename str              = Llama-3.1
llama_model_loader: - kv   6:                         general.size_label str              = 70B
llama_model_loader: - kv   7:                            general.license str              = llama3.1
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Llama 3.1 70B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Meta Llama
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/meta-llama/Lla...
llama_model_loader: - kv  12:                               general.tags arr[str,5]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv  13:                          general.languages arr[str,7]       = ["fr", "it", "pt", "hi", "es", "th", ...
llama_model_loader: - kv  14:                          llama.block_count u32              = 80
llama_model_loader: - kv  15:                       llama.context_length u32              = 131072
llama_model_loader: - kv  16:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  17:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  18:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  19:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  20:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  21:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  22:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  23:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  24:                          general.file_type u32              = 15
llama_model_loader: - kv  25:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  26:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  27:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  28:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  29:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  30:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  31:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  32:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  33:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
time=2024-12-08T07:23:59.403-05:00 level=INFO source=server.go:596 msg="waiting for server to become available" status="llm server loading model"
llm_load_vocab: special tokens cache size = 256
llm_load_vocab: token to piece cache size = 0.7999 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 8192
llm_load_print_meta: n_layer          = 80
llm_load_print_meta: n_head           = 64
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 8
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 28672
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 70B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 70.55 B
llm_load_print_meta: model size       = 39.59 GiB (4.82 BPW) 
llm_load_print_meta: general.name     = Llama 3.1 70B Instruct 2024 12
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'
llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'
llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'
llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'
llm_load_print_meta: max token length = 256
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA H100 80GB HBM3, compute capability 9.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.68 MiB
time=2024-12-08T07:24:03.615-05:00 level=INFO source=server.go:596 msg="waiting for server to become available" status="llm server not responding"
time=2024-12-08T07:24:04.467-05:00 level=INFO source=server.go:596 msg="waiting for server to become available" status="llm server loading model"
llm_load_tensors: offloading 80 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 81/81 layers to GPU
llm_load_tensors:        CPU buffer size =   563.62 MiB
llm_load_tensors:      CUDA0 buffer size = 39979.50 MiB
llama_new_context_with_model: n_ctx      = 8192
llama_new_context_with_model: n_batch    = 2048
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2560.00 MiB
llama_new_context_with_model: KV self size  = 2560.00 MiB, K (f16): 1280.00 MiB, V (f16): 1280.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     2.08 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =  1104.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB
llama_new_context_with_model: graph nodes  = 2566
llama_new_context_with_model: graph splits = 2
time=2024-12-08T07:24:08.979-05:00 level=INFO source=server.go:601 msg="llama runner started in 9.83 seconds"
llama_model_loader: loaded meta data with 36 key-value pairs and 724 tensors from /home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-4824460d29f2058aaf6e1118a63a7a197a09bed509f0e7d4e2efb1ee273b447d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.1 70B Instruct 2024 12
llama_model_loader: - kv   3:                            general.version str              = 2024-12
llama_model_loader: - kv   4:                           general.finetune str              = Instruct
llama_model_loader: - kv   5:                           general.basename str              = Llama-3.1
llama_model_loader: - kv   6:                         general.size_label str              = 70B
llama_model_loader: - kv   7:                            general.license str              = llama3.1
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Llama 3.1 70B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Meta Llama
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/meta-llama/Lla...
llama_model_loader: - kv  12:                               general.tags arr[str,5]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv  13:                          general.languages arr[str,7]       = ["fr", "it", "pt", "hi", "es", "th", ...
llama_model_loader: - kv  14:                          llama.block_count u32              = 80
llama_model_loader: - kv  15:                       llama.context_length u32              = 131072
llama_model_loader: - kv  16:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  17:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  18:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  19:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  20:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  21:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  22:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  23:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  24:                          general.file_type u32              = 15
llama_model_loader: - kv  25:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  26:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  27:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  28:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  29:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  30:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  31:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  32:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  33:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
llm_load_vocab: special tokens cache size = 256
llm_load_vocab: token to piece cache size = 0.7999 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 70.55 B
llm_load_print_meta: model size       = 39.59 GiB (4.82 BPW) 
llm_load_print_meta: general.name     = Llama 3.1 70B Instruct 2024 12
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'
llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'
llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'
llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'
llm_load_print_meta: max token length = 256
llama_model_load: vocab only - skipping tensors
[GIN] 2024/12/08 - 07:24:11 | 200 | 25.061699859s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:24:11 | 404 |     104.921µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:24:11 | 404 |      72.978µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:24:13 | 200 |  1.316019588s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:24:13 | 404 |      93.534µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:24:13 | 404 |      52.204µs |       127.0.0.1 | POST     "/api/show"
time=2024-12-08T07:24:13.362-05:00 level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-94395144-5534-7417-5dae-bdd2d128f6ba library=cuda total="79.1 GiB" available="35.3 GiB"
time=2024-12-08T07:24:13.363-05:00 level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-819c2adf5ce6df2b6bd2ae4ca90d2a69f060afeb438d0c171db57daa02e39c3d gpu=GPU-94395144-5534-7417-5dae-bdd2d128f6ba parallel=1 available=37849726976 required="1.1 GiB"
time=2024-12-08T07:24:13.631-05:00 level=INFO source=server.go:105 msg="system memory" total="2015.0 GiB" free="1977.7 GiB" free_swap="7.9 GiB"
time=2024-12-08T07:24:13.631-05:00 level=INFO source=memory.go:343 msg="offload to cuda" layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[35.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.1 GiB" memory.required.partial="1.1 GiB" memory.required.kv="3.0 MiB" memory.required.allocations="[1.1 GiB]" memory.weights.total="580.2 MiB" memory.weights.repeating="520.6 MiB" memory.weights.nonrepeating="59.6 MiB" memory.graph.full="8.0 MiB" memory.graph.partial="8.0 MiB"
time=2024-12-08T07:24:13.632-05:00 level=INFO source=server.go:383 msg="starting llama server" cmd="/scratch/1030516/ollama3367164927/runners/cuda_v12/ollama_llama_server --model /home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-819c2adf5ce6df2b6bd2ae4ca90d2a69f060afeb438d0c171db57daa02e39c3d --ctx-size 512 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 45623"
time=2024-12-08T07:24:13.632-05:00 level=INFO source=sched.go:449 msg="loaded runners" count=2
time=2024-12-08T07:24:13.632-05:00 level=INFO source=server.go:562 msg="waiting for llama runner to start responding"
time=2024-12-08T07:24:13.633-05:00 level=INFO source=server.go:596 msg="waiting for server to become available" status="llm server error"
time=2024-12-08T07:24:13.669-05:00 level=INFO source=runner.go:863 msg="starting go runner"
time=2024-12-08T07:24:13.669-05:00 level=INFO source=runner.go:864 msg=system info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2024-12-08T07:24:13.669-05:00 level=INFO source=.:0 msg="Server listening on 127.0.0.1:45623"
llama_model_loader: loaded meta data with 23 key-value pairs and 389 tensors from /home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-819c2adf5ce6df2b6bd2ae4ca90d2a69f060afeb438d0c171db57daa02e39c3d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = mxbai-embed-large-v1
llama_model_loader: - kv   2:                           bert.block_count u32              = 24
llama_model_loader: - kv   3:                        bert.context_length u32              = 512
llama_model_loader: - kv   4:                      bert.embedding_length u32              = 1024
llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 4096
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 16
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                      bert.attention.causal bool             = false
llama_model_loader: - kv  10:                          bert.pooling_type u32              = 2
llama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  21:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:  243 tensors
llama_model_loader: - type  f16:  146 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 512
llm_load_print_meta: n_embd           = 1024
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 4096
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 2
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 512
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 335M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 334.09 M
llm_load_print_meta: model size       = 637.85 MiB (16.02 BPW) 
llm_load_print_meta: general.name     = mxbai-embed-large-v1
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA H100 80GB HBM3, compute capability 9.0, VMM: yes
time=2024-12-08T07:24:13.883-05:00 level=INFO source=server.go:596 msg="waiting for server to become available" status="llm server loading model"
llm_load_tensors: ggml ctx size =    0.32 MiB
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors:        CPU buffer size =    60.62 MiB
llm_load_tensors:      CUDA0 buffer size =   577.23 MiB
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =    48.00 MiB
llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    25.01 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     5.01 MiB
llama_new_context_with_model: graph nodes  = 849
llama_new_context_with_model: graph splits = 2
time=2024-12-08T07:24:14.134-05:00 level=INFO source=server.go:601 msg="llama runner started in 0.50 seconds"
llama_model_loader: loaded meta data with 23 key-value pairs and 389 tensors from /home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-819c2adf5ce6df2b6bd2ae4ca90d2a69f060afeb438d0c171db57daa02e39c3d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = mxbai-embed-large-v1
llama_model_loader: - kv   2:                           bert.block_count u32              = 24
llama_model_loader: - kv   3:                        bert.context_length u32              = 512
llama_model_loader: - kv   4:                      bert.embedding_length u32              = 1024
llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 4096
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 16
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                      bert.attention.causal bool             = false
llama_model_loader: - kv  10:                          bert.pooling_type u32              = 2
llama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  21:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:  243 tensors
llama_model_loader: - type  f16:  146 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 334.09 M
llm_load_print_meta: model size       = 637.85 MiB (16.02 BPW) 
llm_load_print_meta: general.name     = mxbai-embed-large-v1
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llama_model_load: vocab only - skipping tensors
[GIN] 2024/12/08 - 07:24:14 | 200 |  1.241592803s |       127.0.0.1 | POST     "/api/embed"
[GIN] 2024/12/08 - 07:24:14 | 404 |     141.129µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:24:14 | 404 |      65.832µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:24:14 | 404 |      34.616µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:24:27 | 200 | 13.069224974s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:24:27 | 404 |     167.939µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:24:27 | 404 |     200.437µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:24:29 | 200 | 14.900327024s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:24:29 | 404 |     808.479µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:24:29 | 404 |     151.048µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:24:31 | 200 | 17.067435336s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:24:31 | 404 |     714.083µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:24:31 | 404 |      19.167µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:24:34 | 200 |  19.71948366s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:24:34 | 404 |      91.658µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:24:34 | 404 |     718.658µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:24:41 | 200 | 14.596578459s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:24:41 | 404 |      98.454µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:24:41 | 404 |      80.354µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:24:45 | 200 | 16.235013341s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:24:45 | 404 |     635.996µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:24:45 | 404 |      97.739µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:24:47 | 200 | 13.788801552s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:24:47 | 404 |     995.515µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:24:47 | 404 |    1.166045ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:24:48 | 200 |  17.41623996s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:24:48 | 404 |     188.132µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:24:48 | 404 |     140.532µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:24:52 | 200 | 10.823221471s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:24:52 | 404 |     623.938µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:24:52 | 404 |     512.605µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:24:56 | 200 | 10.990442114s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:24:56 | 404 |     719.462µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:24:56 | 404 |      75.805µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:24:59 | 200 |   3.30193762s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:24:59 | 404 |     221.238µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:24:59 | 404 |      85.604µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:24:59 | 404 |      34.129µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:25:28 | 200 | 28.697346125s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:25:28 | 404 |     101.427µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:25:28 | 404 |      72.233µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:25:28 | 404 |      73.989µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:25:31 | 200 |   3.23235352s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:25:31 | 404 |     214.753µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:25:31 | 404 |      98.644µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:25:33 | 200 |  2.106249425s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:25:33 | 404 |      90.925µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:25:33 | 404 |      90.305µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:25:33 | 200 |   16.031999ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2024/12/08 - 07:25:33 | 404 |      868.65µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:25:33 | 404 |     421.025µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:25:33 | 404 |      36.553µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:25:48 | 200 | 14.435039998s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:25:48 | 404 |     608.094µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:25:48 | 404 |     572.319µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:25:50 | 200 | 16.341077765s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:25:50 | 404 |      217.54µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:25:50 | 404 |     215.756µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:25:52 | 200 | 18.454333228s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:25:52 | 404 |     610.622µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:25:52 | 404 |     470.871µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:25:56 | 200 | 22.976436138s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:25:56 | 404 |      754.26µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:25:56 | 404 |     142.842µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:26:03 | 200 | 14.932816067s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:26:03 | 404 |     174.534µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:26:03 | 404 |      32.001µs |       127.0.0.1 | POST     "/api/show"
time=2024-12-08T07:26:04.252-05:00 level=ERROR source=runner.go:426 msg="failed to decode batch" error="could not find a KV slot for the batch - try reducing the size of the batch or increase the context. code: 1"
llama_get_logits_ith: invalid logits id 514, reason: batch.logits[514] != true
SIGSEGV: segmentation violation
PC=0x555555a5ec68 m=4 sigcode=1 addr=0x0
signal arrived during cgo execution

goroutine 5 gp=0xc000166000 m=4 mp=0xc000069808 [syscall]:
runtime.cgocall(0x5555558d6cc0, 0xc0000c9c78)
	runtime/cgocall.go:157 +0x4b fp=0xc0000c9c50 sp=0xc0000c9c18 pc=0x5555556593cb
github.com/ollama/ollama/llama._Cfunc_gpt_sampler_csample(0x155488199320, 0x15548c0072e0, 0x202)
	_cgo_gotypes.go:463 +0x4f fp=0xc0000c9c78 sp=0xc0000c9c50 pc=0x5555557564af
main.(*Server).processBatch.(*SamplingContext).Sample.func2(0xc000228500?, 0x2?, 0x202)
	github.com/ollama/ollama/llama/llama.go:663 +0x86 fp=0xc0000c9cc8 sp=0xc0000c9c78 pc=0x5555558d2ca6
github.com/ollama/ollama/llama.(*SamplingContext).Sample(...)
	github.com/ollama/ollama/llama/llama.go:663
main.(*Server).processBatch(0xc00012c120, 0xc0000d0000, 0xc0000c9f10)
	github.com/ollama/ollama/llama/runner/runner.go:458 +0x4ea fp=0xc0000c9ed0 sp=0xc0000c9cc8 pc=0x5555558d1fca
main.(*Server).run(0xc00012c120, {0x555555c10a40, 0xc00007e050})
	github.com/ollama/ollama/llama/runner/runner.go:338 +0x1a5 fp=0xc0000c9fb8 sp=0xc0000c9ed0 pc=0x5555558d1765
main.main.gowrap2()
	github.com/ollama/ollama/llama/runner/runner.go:901 +0x28 fp=0xc0000c9fe0 sp=0xc0000c9fb8 pc=0x5555558d5ec8
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc0000c9fe8 sp=0xc0000c9fe0 pc=0x5555556c1de1
created by main.main in goroutine 1
	github.com/ollama/ollama/llama/runner/runner.go:901 +0xc2b

goroutine 1 gp=0xc0000061c0 m=nil [IO wait, 1 minutes]:
runtime.gopark(0xc000050008?, 0x0?, 0xc0?, 0x61?, 0xc0001578c0?)
	runtime/proc.go:402 +0xce fp=0xc000157888 sp=0xc000157868 pc=0x55555569000e
runtime.netpollblock(0xc000157920?, 0x55658b26?, 0x55?)
	runtime/netpoll.go:573 +0xf7 fp=0xc0001578c0 sp=0xc000157888 pc=0x555555688257
internal/poll.runtime_pollWait(0x1554e4e19820, 0x72)
	runtime/netpoll.go:345 +0x85 fp=0xc0001578e0 sp=0xc0001578c0 pc=0x5555556bcaa5
internal/poll.(*pollDesc).wait(0x3?, 0x1?, 0x0)
	internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc000157908 sp=0xc0001578e0 pc=0x55555570c9c7
internal/poll.(*pollDesc).waitRead(...)
	internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Accept(0xc000160080)
	internal/poll/fd_unix.go:611 +0x2ac fp=0xc0001579b0 sp=0xc000157908 pc=0x55555570de8c
net.(*netFD).accept(0xc000160080)
	net/fd_unix.go:172 +0x29 fp=0xc000157a68 sp=0xc0001579b0 pc=0x55555577c8a9
net.(*TCPListener).accept(0xc00004e200)
	net/tcpsock_posix.go:159 +0x1e fp=0xc000157a90 sp=0xc000157a68 pc=0x55555578d5de
net.(*TCPListener).Accept(0xc00004e200)
	net/tcpsock.go:327 +0x30 fp=0xc000157ac0 sp=0xc000157a90 pc=0x55555578c930
net/http.(*onceCloseListener).Accept(0xc000146240?)
	<autogenerated>:1 +0x24 fp=0xc000157ad8 sp=0xc000157ac0 pc=0x5555558b3a44
net/http.(*Server).Serve(0xc0000161e0, {0x555555c10400, 0xc00004e200})
	net/http/server.go:3260 +0x33e fp=0xc000157c08 sp=0xc000157ad8 pc=0x5555558aa85e
main.main()
	github.com/ollama/ollama/llama/runner/runner.go:921 +0xfcc fp=0xc000157f50 sp=0xc000157c08 pc=0x5555558d5c4c
runtime.main()
	runtime/proc.go:271 +0x29d fp=0xc000157fe0 sp=0xc000157f50 pc=0x55555568fbdd
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc000157fe8 sp=0xc000157fe0 pc=0x5555556c1de1

goroutine 2 gp=0xc000006c40 m=nil [force gc (idle), 1 minutes]:
runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:402 +0xce fp=0xc000062fa8 sp=0xc000062f88 pc=0x55555569000e
runtime.goparkunlock(...)
	runtime/proc.go:408
runtime.forcegchelper()
	runtime/proc.go:326 +0xb8 fp=0xc000062fe0 sp=0xc000062fa8 pc=0x55555568fe98
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc000062fe8 sp=0xc000062fe0 pc=0x5555556c1de1
created by runtime.init.6 in goroutine 1
	runtime/proc.go:314 +0x1a

goroutine 18 gp=0xc00009a380 m=nil [GC sweep wait]:
runtime.gopark(0x1?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:402 +0xce fp=0xc00005e780 sp=0xc00005e760 pc=0x55555569000e
runtime.goparkunlock(...)
	runtime/proc.go:408
runtime.bgsweep(0xc00007a000)
	runtime/mgcsweep.go:318 +0xdf fp=0xc00005e7c8 sp=0xc00005e780 pc=0x55555567ab9f
runtime.gcenable.gowrap1()
	runtime/mgc.go:203 +0x25 fp=0xc00005e7e0 sp=0xc00005e7c8 pc=0x55555566f685
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc00005e7e8 sp=0xc00005e7e0 pc=0x5555556c1de1
created by runtime.gcenable in goroutine 1
	runtime/mgc.go:203 +0x66

goroutine 19 gp=0xc00009a540 m=nil [GC scavenge wait]:
runtime.gopark(0x10000?, 0x3b9aca00?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:402 +0xce fp=0xc00005ef78 sp=0xc00005ef58 pc=0x55555569000e
runtime.goparkunlock(...)
	runtime/proc.go:408
runtime.(*scavengerState).park(0x555555dde4c0)
	runtime/mgcscavenge.go:425 +0x49 fp=0xc00005efa8 sp=0xc00005ef78 pc=0x555555678549
runtime.bgscavenge(0xc00007a000)
	runtime/mgcscavenge.go:658 +0x59 fp=0xc00005efc8 sp=0xc00005efa8 pc=0x555555678af9
runtime.gcenable.gowrap2()
	runtime/mgc.go:204 +0x25 fp=0xc00005efe0 sp=0xc00005efc8 pc=0x55555566f625
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc00005efe8 sp=0xc00005efe0 pc=0x5555556c1de1
created by runtime.gcenable in goroutine 1
	runtime/mgc.go:204 +0xa5

goroutine 3 gp=0xc000007c00 m=nil [finalizer wait]:
runtime.gopark(0x0?, 0x555555c0c1a0?, 0x10?, 0x0?, 0x1000000010?)
	runtime/proc.go:402 +0xce fp=0xc000062620 sp=0xc000062600 pc=0x55555569000e
runtime.runfinq()
	runtime/mfinal.go:194 +0x107 fp=0xc0000627e0 sp=0xc000062620 pc=0x55555566e6c7
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc0000627e8 sp=0xc0000627e0 pc=0x5555556c1de1
created by runtime.createfing in goroutine 1
	runtime/mfinal.go:164 +0x3d

goroutine 122 gp=0xc000196000 m=nil [select]:
runtime.gopark(0xc0000cba80?, 0x2?, 0x18?, 0xb7?, 0xc0000cb824?)
	runtime/proc.go:402 +0xce fp=0xc0000cb698 sp=0xc0000cb678 pc=0x55555569000e
runtime.selectgo(0xc0000cba80, 0xc0000cb820, 0xc0000b7980?, 0x0, 0x1?, 0x1)
	runtime/select.go:327 +0x725 fp=0xc0000cb7b8 sp=0xc0000cb698 pc=0x5555556a13e5
main.(*Server).completion(0xc00012c120, {0x555555c105b0, 0xc00017e0e0}, 0xc000422120)
	github.com/ollama/ollama/llama/runner/runner.go:652 +0x8fe fp=0xc0000cbab8 sp=0xc0000cb7b8 pc=0x5555558d36de
main.(*Server).completion-fm({0x555555c105b0?, 0xc00017e0e0?}, 0x5555558aeb8d?)
	<autogenerated>:1 +0x36 fp=0xc0000cbae8 sp=0xc0000cbab8 pc=0x5555558d66b6
net/http.HandlerFunc.ServeHTTP(0xc000115380?, {0x555555c105b0?, 0xc00017e0e0?}, 0x10?)
	net/http/server.go:2171 +0x29 fp=0xc0000cbb10 sp=0xc0000cbae8 pc=0x5555558a7629
net/http.(*ServeMux).ServeHTTP(0x555555662f85?, {0x555555c105b0, 0xc00017e0e0}, 0xc000422120)
	net/http/server.go:2688 +0x1ad fp=0xc0000cbb60 sp=0xc0000cbb10 pc=0x5555558a94ad
net/http.serverHandler.ServeHTTP({0x555555c0f900?}, {0x555555c105b0?, 0xc00017e0e0?}, 0x6?)
	net/http/server.go:3142 +0x8e fp=0xc0000cbb90 sp=0xc0000cbb60 pc=0x5555558aa4ce
net/http.(*conn).serve(0xc000146240, {0x555555c10a08, 0xc000112db0})
	net/http/server.go:2044 +0x5e8 fp=0xc0000cbfb8 sp=0xc0000cbb90 pc=0x5555558a6268
net/http.(*Server).Serve.gowrap3()
	net/http/server.go:3290 +0x28 fp=0xc0000cbfe0 sp=0xc0000cbfb8 pc=0x5555558aac48
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc0000cbfe8 sp=0xc0000cbfe0 pc=0x5555556c1de1
created by net/http.(*Server).Serve in goroutine 1
	net/http/server.go:3290 +0x4b4

goroutine 110 gp=0xc0001961c0 m=nil [IO wait, 1 minutes]:
runtime.gopark(0x10?, 0x10?, 0xf0?, 0xd?, 0xb?)
	runtime/proc.go:402 +0xce fp=0xc000400da8 sp=0xc000400d88 pc=0x55555569000e
runtime.netpollblock(0x5555556f6558?, 0x55658b26?, 0x55?)
	runtime/netpoll.go:573 +0xf7 fp=0xc000400de0 sp=0xc000400da8 pc=0x555555688257
internal/poll.runtime_pollWait(0x1554e4e19728, 0x72)
	runtime/netpoll.go:345 +0x85 fp=0xc000400e00 sp=0xc000400de0 pc=0x5555556bcaa5
internal/poll.(*pollDesc).wait(0xc000161180?, 0xc0000aca61?, 0x0)
	internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc000400e28 sp=0xc000400e00 pc=0x55555570c9c7
internal/poll.(*pollDesc).waitRead(...)
	internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Read(0xc000161180, {0xc0000aca61, 0x1, 0x1})
	internal/poll/fd_unix.go:164 +0x27a fp=0xc000400ec0 sp=0xc000400e28 pc=0x55555570d51a
net.(*netFD).Read(0xc000161180, {0xc0000aca61?, 0xc000400f48?, 0x5555556be6d0?})
	net/fd_posix.go:55 +0x25 fp=0xc000400f08 sp=0xc000400ec0 pc=0x55555577b7a5
net.(*conn).Read(0xc000066138, {0xc0000aca61?, 0x0?, 0x555555ec7060?})
	net/net.go:185 +0x45 fp=0xc000400f50 sp=0xc000400f08 pc=0x555555785a65
net.(*TCPConn).Read(0x555555d9f840?, {0xc0000aca61?, 0x0?, 0x0?})
	<autogenerated>:1 +0x25 fp=0xc000400f80 sp=0xc000400f50 pc=0x555555791445
net/http.(*connReader).backgroundRead(0xc0000aca50)
	net/http/server.go:681 +0x37 fp=0xc000400fc8 sp=0xc000400f80 pc=0x5555558a01d7
net/http.(*connReader).startBackgroundRead.gowrap2()
	net/http/server.go:677 +0x25 fp=0xc000400fe0 sp=0xc000400fc8 pc=0x5555558a0105
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc000400fe8 sp=0xc000400fe0 pc=0x5555556c1de1
created by net/http.(*connReader).startBackgroundRead in goroutine 73
	net/http/server.go:677 +0xba

goroutine 98 gp=0xc000196380 m=nil [GC worker (idle), 1 minutes]:
runtime.gopark(0x1b09110f05a57b?, 0xc00013ad10?, 0x0?, 0x0?, 0x555555ec7060?)
	runtime/proc.go:402 +0xce fp=0xc0001be750 sp=0xc0001be730 pc=0x55555569000e
runtime.gcBgMarkWorker()
	runtime/mgc.go:1310 +0xe5 fp=0xc0001be7e0 sp=0xc0001be750 pc=0x555555671585
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc0001be7e8 sp=0xc0001be7e0 pc=0x5555556c1de1
created by runtime.gcBgMarkStartWorkers in goroutine 73
	runtime/mgc.go:1234 +0x1c

goroutine 73 gp=0xc000218000 m=nil [select]:
runtime.gopark(0xc000155a80?, 0x2?, 0x18?, 0x57?, 0xc000155824?)
	runtime/proc.go:402 +0xce fp=0xc000155698 sp=0xc000155678 pc=0x55555569000e
runtime.selectgo(0xc000155a80, 0xc000155820, 0xc0000b7c00?, 0x0, 0x1?, 0x1)
	runtime/select.go:327 +0x725 fp=0xc0001557b8 sp=0xc000155698 pc=0x5555556a13e5
main.(*Server).completion(0xc00012c120, {0x555555c105b0, 0xc0000b2700}, 0xc00019eb40)
	github.com/ollama/ollama/llama/runner/runner.go:652 +0x8fe fp=0xc000155ab8 sp=0xc0001557b8 pc=0x5555558d36de
main.(*Server).completion-fm({0x555555c105b0?, 0xc0000b2700?}, 0x5555558aeb8d?)
	<autogenerated>:1 +0x36 fp=0xc000155ae8 sp=0xc000155ab8 pc=0x5555558d66b6
net/http.HandlerFunc.ServeHTTP(0xc000115380?, {0x555555c105b0?, 0xc0000b2700?}, 0x10?)
	net/http/server.go:2171 +0x29 fp=0xc000155b10 sp=0xc000155ae8 pc=0x5555558a7629
net/http.(*ServeMux).ServeHTTP(0x555555662f85?, {0x555555c105b0, 0xc0000b2700}, 0xc00019eb40)
	net/http/server.go:2688 +0x1ad fp=0xc000155b60 sp=0xc000155b10 pc=0x5555558a94ad
net/http.serverHandler.ServeHTTP({0x555555c0f900?}, {0x555555c105b0?, 0xc0000b2700?}, 0x6?)
	net/http/server.go:3142 +0x8e fp=0xc000155b90 sp=0xc000155b60 pc=0x5555558aa4ce
net/http.(*conn).serve(0xc00012cfc0, {0x555555c10a08, 0xc000112db0})
	net/http/server.go:2044 +0x5e8 fp=0xc000155fb8 sp=0xc000155b90 pc=0x5555558a6268
net/http.(*Server).Serve.gowrap3()
	net/http/server.go:3290 +0x28 fp=0xc000155fe0 sp=0xc000155fb8 pc=0x5555558aac48
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc000155fe8 sp=0xc000155fe0 pc=0x5555556c1de1
created by net/http.(*Server).Serve in goroutine 1
	net/http/server.go:3290 +0x4b4

goroutine 126 gp=0xc000196540 m=nil [IO wait]:
runtime.gopark(0x10?, 0x10?, 0xf0?, 0x15?, 0xb?)
	runtime/proc.go:402 +0xce fp=0xc0001c15a8 sp=0xc0001c1588 pc=0x55555569000e
runtime.netpollblock(0x5555556f6558?, 0x55658b26?, 0x55?)
	runtime/netpoll.go:573 +0xf7 fp=0xc0001c15e0 sp=0xc0001c15a8 pc=0x555555688257
internal/poll.runtime_pollWait(0x1554e4e19440, 0x72)
	runtime/netpoll.go:345 +0x85 fp=0xc0001c1600 sp=0xc0001c15e0 pc=0x5555556bcaa5
internal/poll.(*pollDesc).wait(0xc000204500?, 0xc000200491?, 0x0)
	internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc0001c1628 sp=0xc0001c1600 pc=0x55555570c9c7
internal/poll.(*pollDesc).waitRead(...)
	internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Read(0xc000204500, {0xc000200491, 0x1, 0x1})
	internal/poll/fd_unix.go:164 +0x27a fp=0xc0001c16c0 sp=0xc0001c1628 pc=0x55555570d51a
net.(*netFD).Read(0xc000204500, {0xc000200491?, 0xc0001c1748?, 0x5555556be6d0?})
	net/fd_posix.go:55 +0x25 fp=0xc0001c1708 sp=0xc0001c16c0 pc=0x55555577b7a5
net.(*conn).Read(0xc000066090, {0xc000200491?, 0x0?, 0x555555ec7060?})
	net/net.go:185 +0x45 fp=0xc0001c1750 sp=0xc0001c1708 pc=0x555555785a65
net.(*TCPConn).Read(0x555555d9f840?, {0xc000200491?, 0x0?, 0x0?})
	<autogenerated>:1 +0x25 fp=0xc0001c1780 sp=0xc0001c1750 pc=0x555555791445
net/http.(*connReader).backgroundRead(0xc000200480)
	net/http/server.go:681 +0x37 fp=0xc0001c17c8 sp=0xc0001c1780 pc=0x5555558a01d7
net/http.(*connReader).startBackgroundRead.gowrap2()
	net/http/server.go:677 +0x25 fp=0xc0001c17e0 sp=0xc0001c17c8 pc=0x5555558a0105
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc0001c17e8 sp=0xc0001c17e0 pc=0x5555556c1de1
created by net/http.(*connReader).startBackgroundRead in goroutine 122
	net/http/server.go:677 +0xba

goroutine 47 gp=0xc000196e00 m=nil [select]:
runtime.gopark(0xc0000c5a80?, 0x2?, 0x18?, 0x57?, 0xc0000c5824?)
	runtime/proc.go:402 +0xce fp=0xc0000c5698 sp=0xc0000c5678 pc=0x55555569000e
runtime.selectgo(0xc0000c5a80, 0xc0000c5820, 0xc0000b7c80?, 0x0, 0x1?, 0x1)
	runtime/select.go:327 +0x725 fp=0xc0000c57b8 sp=0xc0000c5698 pc=0x5555556a13e5
main.(*Server).completion(0xc00012c120, {0x555555c105b0, 0xc00017e2a0}, 0xc000422360)
	github.com/ollama/ollama/llama/runner/runner.go:652 +0x8fe fp=0xc0000c5ab8 sp=0xc0000c57b8 pc=0x5555558d36de
main.(*Server).completion-fm({0x555555c105b0?, 0xc00017e2a0?}, 0x5555558aeb8d?)
	<autogenerated>:1 +0x36 fp=0xc0000c5ae8 sp=0xc0000c5ab8 pc=0x5555558d66b6
net/http.HandlerFunc.ServeHTTP(0xc000115380?, {0x555555c105b0?, 0xc00017e2a0?}, 0x10?)
	net/http/server.go:2171 +0x29 fp=0xc0000c5b10 sp=0xc0000c5ae8 pc=0x5555558a7629
net/http.(*ServeMux).ServeHTTP(0x555555662f85?, {0x555555c105b0, 0xc00017e2a0}, 0xc000422360)
	net/http/server.go:2688 +0x1ad fp=0xc0000c5b60 sp=0xc0000c5b10 pc=0x5555558a94ad
net/http.serverHandler.ServeHTTP({0x555555c0f900?}, {0x555555c105b0?, 0xc00017e2a0?}, 0x6?)
	net/http/server.go:3142 +0x8e fp=0xc0000c5b90 sp=0xc0000c5b60 pc=0x5555558aa4ce
net/http.(*conn).serve(0xc000190990, {0x555555c10a08, 0xc000112db0})
	net/http/server.go:2044 +0x5e8 fp=0xc0000c5fb8 sp=0xc0000c5b90 pc=0x5555558a6268
net/http.(*Server).Serve.gowrap3()
	net/http/server.go:3290 +0x28 fp=0xc0000c5fe0 sp=0xc0000c5fb8 pc=0x5555558aac48
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc0000c5fe8 sp=0xc0000c5fe0 pc=0x5555556c1de1
created by net/http.(*Server).Serve in goroutine 1
	net/http/server.go:3290 +0x4b4

goroutine 106 gp=0xc0001661c0 m=nil [select]:
runtime.gopark(0xc000159a80?, 0x2?, 0x18?, 0x97?, 0xc000159824?)
	runtime/proc.go:402 +0xce fp=0xc000159698 sp=0xc000159678 pc=0x55555569000e
runtime.selectgo(0xc000159a80, 0xc000159820, 0xc0000b7b80?, 0x0, 0x1?, 0x1)
	runtime/select.go:327 +0x725 fp=0xc0001597b8 sp=0xc000159698 pc=0x5555556a13e5
main.(*Server).completion(0xc00012c120, {0x555555c105b0, 0xc00017e460}, 0xc0004226c0)
	github.com/ollama/ollama/llama/runner/runner.go:652 +0x8fe fp=0xc000159ab8 sp=0xc0001597b8 pc=0x5555558d36de
main.(*Server).completion-fm({0x555555c105b0?, 0xc00017e460?}, 0x5555558aeb8d?)
	<autogenerated>:1 +0x36 fp=0xc000159ae8 sp=0xc000159ab8 pc=0x5555558d66b6
net/http.HandlerFunc.ServeHTTP(0xc000115380?, {0x555555c105b0?, 0xc00017e460?}, 0x10?)
	net/http/server.go:2171 +0x29 fp=0xc000159b10 sp=0xc000159ae8 pc=0x5555558a7629
net/http.(*ServeMux).ServeHTTP(0x555555662f85?, {0x555555c105b0, 0xc00017e460}, 0xc0004226c0)
	net/http/server.go:2688 +0x1ad fp=0xc000159b60 sp=0xc000159b10 pc=0x5555558a94ad
net/http.serverHandler.ServeHTTP({0x555555c0f900?}, {0x555555c105b0?, 0xc00017e460?}, 0x6?)
	net/http/server.go:3142 +0x8e fp=0xc000159b90 sp=0xc000159b60 pc=0x5555558aa4ce
net/http.(*conn).serve(0xc0001902d0, {0x555555c10a08, 0xc000112db0})
	net/http/server.go:2044 +0x5e8 fp=0xc000159fb8 sp=0xc000159b90 pc=0x5555558a6268
net/http.(*Server).Serve.gowrap3()
	net/http/server.go:3290 +0x28 fp=0xc000159fe0 sp=0xc000159fb8 pc=0x5555558aac48
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc000159fe8 sp=0xc000159fe0 pc=0x5555556c1de1
created by net/http.(*Server).Serve in goroutine 1
	net/http/server.go:3290 +0x4b4

goroutine 78 gp=0xc000166380 m=nil [GC worker (idle), 1 minutes]:
runtime.gopark(0x1b08fc4b4055dc?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:402 +0xce fp=0xc000064750 sp=0xc000064730 pc=0x55555569000e
runtime.gcBgMarkWorker()
	runtime/mgc.go:1310 +0xe5 fp=0xc0000647e0 sp=0xc000064750 pc=0x555555671585
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc0000647e8 sp=0xc0000647e0 pc=0x5555556c1de1
created by runtime.gcBgMarkStartWorkers in goroutine 73
	runtime/mgc.go:1234 +0x1c

goroutine 59 gp=0xc0002181c0 m=nil [GC worker (idle)]:
runtime.gopark(0x1b09110f05a488?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:402 +0xce fp=0xc0001bb750 sp=0xc0001bb730 pc=0x55555569000e
runtime.gcBgMarkWorker()
	runtime/mgc.go:1310 +0xe5 fp=0xc0001bb7e0 sp=0xc0001bb750 pc=0x555555671585
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc0001bb7e8 sp=0xc0001bb7e0 pc=0x5555556c1de1
created by runtime.gcBgMarkStartWorkers in goroutine 73
	runtime/mgc.go:1234 +0x1c

goroutine 60 gp=0xc000218380 m=nil [GC worker (idle), 1 minutes]:
runtime.gopark(0x1b09110f05aebf?, 0x0?, 0x0?, 0x0?, 0x0?)
	runtime/proc.go:402 +0xce fp=0xc0001bbf50 sp=0xc0001bbf30 pc=0x55555569000e
runtime.gcBgMarkWorker()
	runtime/mgc.go:1310 +0xe5 fp=0xc0001bbfe0 sp=0xc0001bbf50 pc=0x555555671585
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc0001bbfe8 sp=0xc0001bbfe0 pc=0x5555556c1de1
created by runtime.gcBgMarkStartWorkers in goroutine 73
	runtime/mgc.go:1234 +0x1c

goroutine 128 gp=0xc000197180 m=nil [IO wait]:
runtime.gopark(0x10?, 0x10?, 0xf0?, 0xfd?, 0xb?)
	runtime/proc.go:402 +0xce fp=0xc00005fda8 sp=0xc00005fd88 pc=0x55555569000e
runtime.netpollblock(0x5555556f6558?, 0x55658b26?, 0x55?)
	runtime/netpoll.go:573 +0xf7 fp=0xc00005fde0 sp=0xc00005fda8 pc=0x555555688257
internal/poll.runtime_pollWait(0x1554e4e19630, 0x72)
	runtime/netpoll.go:345 +0x85 fp=0xc00005fe00 sp=0xc00005fde0 pc=0x5555556bcaa5
internal/poll.(*pollDesc).wait(0xc00018f280?, 0xc000184be1?, 0x0)
	internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc00005fe28 sp=0xc00005fe00 pc=0x55555570c9c7
internal/poll.(*pollDesc).waitRead(...)
	internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Read(0xc00018f280, {0xc000184be1, 0x1, 0x1})
	internal/poll/fd_unix.go:164 +0x27a fp=0xc00005fec0 sp=0xc00005fe28 pc=0x55555570d51a
net.(*netFD).Read(0xc00018f280, {0xc000184be1?, 0xc00005ff48?, 0x5555556be6d0?})
	net/fd_posix.go:55 +0x25 fp=0xc00005ff08 sp=0xc00005fec0 pc=0x55555577b7a5
net.(*conn).Read(0xc000186068, {0xc000184be1?, 0x0?, 0x555555ec7060?})
	net/net.go:185 +0x45 fp=0xc00005ff50 sp=0xc00005ff08 pc=0x555555785a65
net.(*TCPConn).Read(0x555555d9f840?, {0xc000184be1?, 0x0?, 0x0?})
	<autogenerated>:1 +0x25 fp=0xc00005ff80 sp=0xc00005ff50 pc=0x555555791445
net/http.(*connReader).backgroundRead(0xc000184bd0)
	net/http/server.go:681 +0x37 fp=0xc00005ffc8 sp=0xc00005ff80 pc=0x5555558a01d7
net/http.(*connReader).startBackgroundRead.gowrap2()
	net/http/server.go:677 +0x25 fp=0xc00005ffe0 sp=0xc00005ffc8 pc=0x5555558a0105
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc00005ffe8 sp=0xc00005ffe0 pc=0x5555556c1de1
created by net/http.(*connReader).startBackgroundRead in goroutine 47
	net/http/server.go:677 +0xba

goroutine 124 gp=0xc00009aa80 m=nil [IO wait, 1 minutes]:
runtime.gopark(0x10?, 0x10?, 0xf0?, 0xad?, 0xb?)
	runtime/proc.go:402 +0xce fp=0xc0001bada8 sp=0xc0001bad88 pc=0x55555569000e
runtime.netpollblock(0x5555556f6558?, 0x55658b26?, 0x55?)
	runtime/netpoll.go:573 +0xf7 fp=0xc0001bade0 sp=0xc0001bada8 pc=0x555555688257
internal/poll.runtime_pollWait(0x1554e4e19538, 0x72)
	runtime/netpoll.go:345 +0x85 fp=0xc0001bae00 sp=0xc0001bade0 pc=0x5555556bcaa5
internal/poll.(*pollDesc).wait(0xc000003f00?, 0xc0000ac461?, 0x0)
	internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc0001bae28 sp=0xc0001bae00 pc=0x55555570c9c7
internal/poll.(*pollDesc).waitRead(...)
	internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Read(0xc000003f00, {0xc0000ac461, 0x1, 0x1})
	internal/poll/fd_unix.go:164 +0x27a fp=0xc0001baec0 sp=0xc0001bae28 pc=0x55555570d51a
net.(*netFD).Read(0xc000003f00, {0xc0000ac461?, 0xc0001baf48?, 0x5555556be6d0?})
	net/fd_posix.go:55 +0x25 fp=0xc0001baf08 sp=0xc0001baec0 pc=0x55555577b7a5
net.(*conn).Read(0xc000186008, {0xc0000ac461?, 0x0?, 0x555555ec7060?})
	net/net.go:185 +0x45 fp=0xc0001baf50 sp=0xc0001baf08 pc=0x555555785a65
net.(*TCPConn).Read(0x555555d9f840?, {0xc0000ac461?, 0x0?, 0x0?})
	<autogenerated>:1 +0x25 fp=0xc0001baf80 sp=0xc0001baf50 pc=0x555555791445
net/http.(*connReader).backgroundRead(0xc0000ac450)
	net/http/server.go:681 +0x37 fp=0xc0001bafc8 sp=0xc0001baf80 pc=0x5555558a01d7
net/http.(*connReader).startBackgroundRead.gowrap2()
	net/http/server.go:677 +0x25 fp=0xc0001bafe0 sp=0xc0001bafc8 pc=0x5555558a0105
runtime.goexit({})
	runtime/asm_amd64.s:1695 +0x1 fp=0xc0001bafe8 sp=0xc0001bafe0 pc=0x5555556c1de1
created by net/http.(*connReader).startBackgroundRead in goroutine 106
	net/http/server.go:677 +0xba

rax    0x0
rbx    0x0
rcx    0x1f500
rdx    0x15524cc90400
rdi    0x15524cc90400
rsi    0x1f500
rbp    0x15548c0072e0
rsp    0x15549dddb7e0
r8     0x1f500
r9     0x15549dddb540
r10    0x1554e4f6d078
r11    0xe0495215bd96d5b3
r12    0x202
r13    0x1554881993f0
r14    0x155488199320
r15    0x0
rip    0x555555a5ec68
rflags 0x10206
cs     0x33
fs     0x0
gs     0x0
[GIN] 2024/12/08 - 07:26:04 | 500 |  7.786705841s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:26:04 | 400 |      43.206µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:26:04 | 400 |      27.876µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:33:48 | 404 |     597.721µs |       127.0.0.1 | POST     "/api/show"
time=2024-12-08T07:33:53.817-05:00 level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=5.20516527 model=/home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-4824460d29f2058aaf6e1118a63a7a197a09bed509f0e7d4e2efb1ee273b447d
time=2024-12-08T07:33:54.073-05:00 level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=5.460607904 model=/home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-4824460d29f2058aaf6e1118a63a7a197a09bed509f0e7d4e2efb1ee273b447d
time=2024-12-08T07:33:54.368-05:00 level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-4824460d29f2058aaf6e1118a63a7a197a09bed509f0e7d4e2efb1ee273b447d gpu=GPU-94395144-5534-7417-5dae-bdd2d128f6ba parallel=4 available=84378779648 required="43.6 GiB"
time=2024-12-08T07:33:54.886-05:00 level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=6.273296657 model=/home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-4824460d29f2058aaf6e1118a63a7a197a09bed509f0e7d4e2efb1ee273b447d
time=2024-12-08T07:33:55.141-05:00 level=INFO source=server.go:105 msg="system memory" total="2015.0 GiB" free="1978.2 GiB" free_swap="7.9 GiB"
time=2024-12-08T07:33:55.142-05:00 level=INFO source=memory.go:343 msg="offload to cuda" layers.requested=-1 layers.model=81 layers.offload=81 layers.split="" memory.available="[78.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="43.6 GiB" memory.required.partial="43.6 GiB" memory.required.kv="2.5 GiB" memory.required.allocations="[43.6 GiB]" memory.weights.total="40.7 GiB" memory.weights.repeating="39.9 GiB" memory.weights.nonrepeating="822.0 MiB" memory.graph.full="1.1 GiB" memory.graph.partial="1.1 GiB"
time=2024-12-08T07:33:55.143-05:00 level=INFO source=server.go:383 msg="starting llama server" cmd="/scratch/1030516/ollama3367164927/runners/cuda_v12/ollama_llama_server --model /home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-4824460d29f2058aaf6e1118a63a7a197a09bed509f0e7d4e2efb1ee273b447d --ctx-size 8192 --batch-size 512 --n-gpu-layers 81 --threads 64 --parallel 4 --port 43661"
time=2024-12-08T07:33:55.144-05:00 level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2024-12-08T07:33:55.144-05:00 level=INFO source=server.go:562 msg="waiting for llama runner to start responding"
time=2024-12-08T07:33:55.144-05:00 level=WARN source=server.go:569 msg="client connection closed before server finished loading, aborting load"
time=2024-12-08T07:33:55.144-05:00 level=ERROR source=sched.go:455 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
[GIN] 2024/12/08 - 07:33:55 | 499 |         7m45s |       127.0.0.1 | POST     "/api/generate"
time=2024-12-08T07:33:55.191-05:00 level=INFO source=runner.go:863 msg="starting go runner"
time=2024-12-08T07:33:55.191-05:00 level=INFO source=runner.go:864 msg=system info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2024-12-08T07:33:55.191-05:00 level=INFO source=.:0 msg="Server listening on 127.0.0.1:43661"
llama_model_loader: loaded meta data with 36 key-value pairs and 724 tensors from /home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-4824460d29f2058aaf6e1118a63a7a197a09bed509f0e7d4e2efb1ee273b447d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.1 70B Instruct 2024 12
llama_model_loader: - kv   3:                            general.version str              = 2024-12
llama_model_loader: - kv   4:                           general.finetune str              = Instruct
llama_model_loader: - kv   5:                           general.basename str              = Llama-3.1
llama_model_loader: - kv   6:                         general.size_label str              = 70B
llama_model_loader: - kv   7:                            general.license str              = llama3.1
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Llama 3.1 70B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Meta Llama
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/meta-llama/Lla...
llama_model_loader: - kv  12:                               general.tags arr[str,5]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv  13:                          general.languages arr[str,7]       = ["fr", "it", "pt", "hi", "es", "th", ...
llama_model_loader: - kv  14:                          llama.block_count u32              = 80
llama_model_loader: - kv  15:                       llama.context_length u32              = 131072
llama_model_loader: - kv  16:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  17:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  18:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  19:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  20:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  21:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  22:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  23:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  24:                          general.file_type u32              = 15
llama_model_loader: - kv  25:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  26:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  27:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  28:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  29:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  30:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  31:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  32:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  33:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
time=2024-12-08T07:34:00.358-05:00 level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=5.213534565 model=/home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-4824460d29f2058aaf6e1118a63a7a197a09bed509f0e7d4e2efb1ee273b447d
time=2024-12-08T07:34:00.614-05:00 level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=5.469392467 model=/home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-4824460d29f2058aaf6e1118a63a7a197a09bed509f0e7d4e2efb1ee273b447d
time=2024-12-08T07:34:00.897-05:00 level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-4824460d29f2058aaf6e1118a63a7a197a09bed509f0e7d4e2efb1ee273b447d gpu=GPU-94395144-5534-7417-5dae-bdd2d128f6ba parallel=4 available=84378779648 required="43.6 GiB"
time=2024-12-08T07:34:01.111-05:00 level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=5.967026904 model=/home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-4824460d29f2058aaf6e1118a63a7a197a09bed509f0e7d4e2efb1ee273b447d
time=2024-12-08T07:34:01.358-05:00 level=INFO source=server.go:105 msg="system memory" total="2015.0 GiB" free="1978.2 GiB" free_swap="7.9 GiB"
time=2024-12-08T07:34:01.359-05:00 level=INFO source=memory.go:343 msg="offload to cuda" layers.requested=-1 layers.model=81 layers.offload=81 layers.split="" memory.available="[78.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="43.6 GiB" memory.required.partial="43.6 GiB" memory.required.kv="2.5 GiB" memory.required.allocations="[43.6 GiB]" memory.weights.total="40.7 GiB" memory.weights.repeating="39.9 GiB" memory.weights.nonrepeating="822.0 MiB" memory.graph.full="1.1 GiB" memory.graph.partial="1.1 GiB"
time=2024-12-08T07:34:01.360-05:00 level=INFO source=server.go:383 msg="starting llama server" cmd="/scratch/1030516/ollama3367164927/runners/cuda_v12/ollama_llama_server --model /home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-4824460d29f2058aaf6e1118a63a7a197a09bed509f0e7d4e2efb1ee273b447d --ctx-size 8192 --batch-size 512 --n-gpu-layers 81 --threads 64 --parallel 4 --port 35047"
time=2024-12-08T07:34:01.360-05:00 level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2024-12-08T07:34:01.360-05:00 level=INFO source=server.go:562 msg="waiting for llama runner to start responding"
time=2024-12-08T07:34:01.360-05:00 level=INFO source=server.go:596 msg="waiting for server to become available" status="llm server error"
time=2024-12-08T07:34:01.396-05:00 level=INFO source=runner.go:863 msg="starting go runner"
time=2024-12-08T07:34:01.396-05:00 level=INFO source=runner.go:864 msg=system info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2024-12-08T07:34:01.396-05:00 level=INFO source=.:0 msg="Server listening on 127.0.0.1:35047"
llama_model_loader: loaded meta data with 36 key-value pairs and 724 tensors from /home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-4824460d29f2058aaf6e1118a63a7a197a09bed509f0e7d4e2efb1ee273b447d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.1 70B Instruct 2024 12
llama_model_loader: - kv   3:                            general.version str              = 2024-12
llama_model_loader: - kv   4:                           general.finetune str              = Instruct
llama_model_loader: - kv   5:                           general.basename str              = Llama-3.1
llama_model_loader: - kv   6:                         general.size_label str              = 70B
llama_model_loader: - kv   7:                            general.license str              = llama3.1
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Llama 3.1 70B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Meta Llama
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/meta-llama/Lla...
llama_model_loader: - kv  12:                               general.tags arr[str,5]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv  13:                          general.languages arr[str,7]       = ["fr", "it", "pt", "hi", "es", "th", ...
llama_model_loader: - kv  14:                          llama.block_count u32              = 80
llama_model_loader: - kv  15:                       llama.context_length u32              = 131072
llama_model_loader: - kv  16:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  17:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  18:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  19:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  20:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  21:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  22:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  23:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  24:                          general.file_type u32              = 15
llama_model_loader: - kv  25:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  26:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  27:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  28:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  29:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  30:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  31:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  32:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  33:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
time=2024-12-08T07:34:01.612-05:00 level=INFO source=server.go:596 msg="waiting for server to become available" status="llm server loading model"
llm_load_vocab: special tokens cache size = 256
llm_load_vocab: token to piece cache size = 0.7999 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 8192
llm_load_print_meta: n_layer          = 80
llm_load_print_meta: n_head           = 64
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 8
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 28672
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 70B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 70.55 B
llm_load_print_meta: model size       = 39.59 GiB (4.82 BPW) 
llm_load_print_meta: general.name     = Llama 3.1 70B Instruct 2024 12
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'
llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'
llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'
llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'
llm_load_print_meta: max token length = 256
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA H100 80GB HBM3, compute capability 9.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.68 MiB
llm_load_tensors: offloading 80 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 81/81 layers to GPU
llm_load_tensors:        CPU buffer size =   563.62 MiB
llm_load_tensors:      CUDA0 buffer size = 39979.50 MiB
llama_new_context_with_model: n_ctx      = 8192
llama_new_context_with_model: n_batch    = 2048
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2560.00 MiB
llama_new_context_with_model: KV self size  = 2560.00 MiB, K (f16): 1280.00 MiB, V (f16): 1280.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     2.08 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =  1104.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    32.01 MiB
llama_new_context_with_model: graph nodes  = 2566
llama_new_context_with_model: graph splits = 2
time=2024-12-08T07:34:11.143-05:00 level=INFO source=server.go:601 msg="llama runner started in 9.78 seconds"
llama_model_loader: loaded meta data with 36 key-value pairs and 724 tensors from /home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-4824460d29f2058aaf6e1118a63a7a197a09bed509f0e7d4e2efb1ee273b447d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.1 70B Instruct 2024 12
llama_model_loader: - kv   3:                            general.version str              = 2024-12
llama_model_loader: - kv   4:                           general.finetune str              = Instruct
llama_model_loader: - kv   5:                           general.basename str              = Llama-3.1
llama_model_loader: - kv   6:                         general.size_label str              = 70B
llama_model_loader: - kv   7:                            general.license str              = llama3.1
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Llama 3.1 70B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Meta Llama
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/meta-llama/Lla...
llama_model_loader: - kv  12:                               general.tags arr[str,5]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv  13:                          general.languages arr[str,7]       = ["fr", "it", "pt", "hi", "es", "th", ...
llama_model_loader: - kv  14:                          llama.block_count u32              = 80
llama_model_loader: - kv  15:                       llama.context_length u32              = 131072
llama_model_loader: - kv  16:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  17:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  18:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  19:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  20:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  21:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  22:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  23:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  24:                          general.file_type u32              = 15
llama_model_loader: - kv  25:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  26:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  27:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  28:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  29:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  30:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  31:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  32:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  33:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
llm_load_vocab: special tokens cache size = 256
llm_load_vocab: token to piece cache size = 0.7999 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 70.55 B
llm_load_print_meta: model size       = 39.59 GiB (4.82 BPW) 
llm_load_print_meta: general.name     = Llama 3.1 70B Instruct 2024 12
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'
llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'
llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'
llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'
llm_load_print_meta: max token length = 256
llama_model_load: vocab only - skipping tensors
[GIN] 2024/12/08 - 07:34:13 | 200 | 24.899595548s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:34:13 | 404 |     215.006µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:34:13 | 404 |     119.366µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:34:14 | 200 |  1.091940848s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:34:14 | 404 |     843.982µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:34:14 | 404 |      86.336µs |       127.0.0.1 | POST     "/api/show"
time=2024-12-08T07:34:14.950-05:00 level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-94395144-5534-7417-5dae-bdd2d128f6ba library=cuda total="79.1 GiB" available="35.3 GiB"
time=2024-12-08T07:34:14.950-05:00 level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-819c2adf5ce6df2b6bd2ae4ca90d2a69f060afeb438d0c171db57daa02e39c3d gpu=GPU-94395144-5534-7417-5dae-bdd2d128f6ba parallel=1 available=37851824128 required="1.1 GiB"
time=2024-12-08T07:34:15.257-05:00 level=INFO source=server.go:105 msg="system memory" total="2015.0 GiB" free="1977.8 GiB" free_swap="7.9 GiB"
time=2024-12-08T07:34:15.257-05:00 level=INFO source=memory.go:343 msg="offload to cuda" layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[35.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.1 GiB" memory.required.partial="1.1 GiB" memory.required.kv="3.0 MiB" memory.required.allocations="[1.1 GiB]" memory.weights.total="580.2 MiB" memory.weights.repeating="520.6 MiB" memory.weights.nonrepeating="59.6 MiB" memory.graph.full="8.0 MiB" memory.graph.partial="8.0 MiB"
time=2024-12-08T07:34:15.258-05:00 level=INFO source=server.go:383 msg="starting llama server" cmd="/scratch/1030516/ollama3367164927/runners/cuda_v12/ollama_llama_server --model /home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-819c2adf5ce6df2b6bd2ae4ca90d2a69f060afeb438d0c171db57daa02e39c3d --ctx-size 512 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 46285"
time=2024-12-08T07:34:15.259-05:00 level=INFO source=sched.go:449 msg="loaded runners" count=2
time=2024-12-08T07:34:15.259-05:00 level=INFO source=server.go:562 msg="waiting for llama runner to start responding"
time=2024-12-08T07:34:15.259-05:00 level=INFO source=server.go:596 msg="waiting for server to become available" status="llm server error"
time=2024-12-08T07:34:15.295-05:00 level=INFO source=runner.go:863 msg="starting go runner"
time=2024-12-08T07:34:15.295-05:00 level=INFO source=runner.go:864 msg=system info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2024-12-08T07:34:15.295-05:00 level=INFO source=.:0 msg="Server listening on 127.0.0.1:46285"
llama_model_loader: loaded meta data with 23 key-value pairs and 389 tensors from /home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-819c2adf5ce6df2b6bd2ae4ca90d2a69f060afeb438d0c171db57daa02e39c3d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = mxbai-embed-large-v1
llama_model_loader: - kv   2:                           bert.block_count u32              = 24
llama_model_loader: - kv   3:                        bert.context_length u32              = 512
llama_model_loader: - kv   4:                      bert.embedding_length u32              = 1024
llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 4096
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 16
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                      bert.attention.causal bool             = false
llama_model_loader: - kv  10:                          bert.pooling_type u32              = 2
llama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  21:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:  243 tensors
llama_model_loader: - type  f16:  146 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 512
llm_load_print_meta: n_embd           = 1024
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 4096
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 2
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 512
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 335M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 334.09 M
llm_load_print_meta: model size       = 637.85 MiB (16.02 BPW) 
llm_load_print_meta: general.name     = mxbai-embed-large-v1
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA H100 80GB HBM3, compute capability 9.0, VMM: yes
time=2024-12-08T07:34:15.510-05:00 level=INFO source=server.go:596 msg="waiting for server to become available" status="llm server loading model"
llm_load_tensors: ggml ctx size =    0.32 MiB
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors:        CPU buffer size =    60.62 MiB
llm_load_tensors:      CUDA0 buffer size =   577.23 MiB
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =    48.00 MiB
llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    25.01 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     5.01 MiB
llama_new_context_with_model: graph nodes  = 849
llama_new_context_with_model: graph splits = 2
time=2024-12-08T07:34:15.762-05:00 level=INFO source=server.go:601 msg="llama runner started in 0.50 seconds"
llama_model_loader: loaded meta data with 23 key-value pairs and 389 tensors from /home/hice1/lpimentel3/scratch/ollama_models/blobs/sha256-819c2adf5ce6df2b6bd2ae4ca90d2a69f060afeb438d0c171db57daa02e39c3d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = mxbai-embed-large-v1
llama_model_loader: - kv   2:                           bert.block_count u32              = 24
llama_model_loader: - kv   3:                        bert.context_length u32              = 512
llama_model_loader: - kv   4:                      bert.embedding_length u32              = 1024
llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 4096
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 16
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                      bert.attention.causal bool             = false
llama_model_loader: - kv  10:                          bert.pooling_type u32              = 2
llama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  21:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:  243 tensors
llama_model_loader: - type  f16:  146 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 334.09 M
llm_load_print_meta: model size       = 637.85 MiB (16.02 BPW) 
llm_load_print_meta: general.name     = mxbai-embed-large-v1
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llama_model_load: vocab only - skipping tensors
[GIN] 2024/12/08 - 07:34:15 | 200 |  1.252569761s |       127.0.0.1 | POST     "/api/embed"
[GIN] 2024/12/08 - 07:34:15 | 404 |     132.444µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:34:15 | 404 |      40.633µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:34:15 | 404 |      78.589µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:34:27 | 200 |  11.22363745s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:34:27 | 404 |     142.976µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:34:27 | 404 |     323.753µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:34:29 | 200 | 13.349966657s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:34:29 | 404 |      80.822µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:34:29 | 404 |     688.578µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:34:31 | 200 |  15.28708617s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:34:31 | 404 |      622.44µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:34:31 | 404 |     572.113µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:34:32 | 200 | 16.087410214s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:34:32 | 404 |        65.2µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:34:32 | 404 |      67.455µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:34:39 | 200 |  12.55544734s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:34:39 | 404 |     169.416µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:34:39 | 404 |     156.567µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:34:43 | 200 | 11.058237422s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:34:43 | 404 |     715.108µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:34:43 | 404 |      68.616µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:34:43 | 200 | 14.540446609s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:34:43 | 404 |     749.478µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:34:43 | 404 |       216.9µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:34:44 | 200 | 13.324791502s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:34:44 | 404 |     572.724µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:34:44 | 404 |     119.909µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:34:46 | 200 |  6.469113568s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:34:46 | 404 |     653.205µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:34:46 | 404 |      19.853µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:34:47 | 200 |  4.288261033s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:34:47 | 404 |     664.512µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:34:47 | 404 |     556.102µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:34:49 | 200 |  2.476652501s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:34:49 | 404 |     584.236µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:34:49 | 404 |       393.2µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:34:49 | 404 |      37.111µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:03 | 200 | 13.215886305s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:35:03 | 404 |     202.141µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:03 | 404 |     168.261µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:03 | 404 |      76.424µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:05 | 200 |  2.230917097s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:35:05 | 404 |     208.854µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:05 | 404 |       88.39µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:06 | 200 |  1.304988017s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:35:06 | 404 |      90.924µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:06 | 404 |     765.609µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:06 | 200 |   17.243832ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2024/12/08 - 07:35:06 | 404 |     131.309µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:06 | 404 |      49.321µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:06 | 404 |      46.796µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:18 | 200 | 11.474920931s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:35:18 | 404 |     129.322µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:18 | 404 |     308.163µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:20 | 200 | 13.764889248s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:35:20 | 404 |     612.371µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:20 | 404 |     755.832µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:23 | 200 |  16.21162212s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:35:23 | 404 |     111.904µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:23 | 404 |     221.844µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:25 | 200 | 18.317522088s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:35:25 | 404 |     622.208µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:25 | 404 |     511.064µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:32 | 200 | 13.894424717s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:35:32 | 404 |     167.614µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:32 | 404 |     173.874µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:33 | 200 | 12.438819964s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:35:33 | 404 |     190.043µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:33 | 404 |      17.517µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:35 | 200 | 10.244002333s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:35:35 | 404 |     611.794µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:35 | 404 |     476.874µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:36 | 200 | 13.867094015s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:35:36 | 404 |      74.905µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:36 | 404 |      56.667µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:39 | 200 |  7.277880169s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:35:39 | 404 |     696.145µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:39 | 404 |      96.711µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:41 | 200 |  8.251718324s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:35:41 | 404 |     102.585µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:41 | 404 |       92.52µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:43 | 200 |  2.616321704s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:35:43 | 404 |     758.216µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:43 | 404 |     106.087µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:43 | 404 |      82.532µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:54 | 200 | 11.019680117s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:35:54 | 404 |     396.156µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:54 | 404 |      96.365µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:55 | 404 |      48.457µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:57 | 200 |  2.366543531s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:35:57 | 404 |     213.166µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:57 | 404 |     105.638µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:58 | 200 |  1.358938013s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:35:58 | 404 |     258.763µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:58 | 404 |      90.342µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:58 | 200 |   17.673709ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2024/12/08 - 07:35:58 | 404 |     139.506µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:58 | 404 |      63.653µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:35:58 | 404 |      48.522µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:10 | 200 | 11.214160824s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:36:10 | 404 |      93.261µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:10 | 404 |      27.913µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:11 | 200 | 13.080397207s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:36:11 | 404 |     761.304µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:11 | 404 |     219.032µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:14 | 200 | 15.436622871s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:36:14 | 404 |     183.264µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:14 | 404 |     107.596µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:17 | 200 | 18.407634522s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:36:17 | 404 |     166.479µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:17 | 404 |     161.893µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:22 | 200 | 10.420757637s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:36:22 | 404 |     177.337µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:22 | 404 |      165.48µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:24 | 200 | 14.644098516s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:36:24 | 404 |      93.713µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:24 | 404 |     738.148µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:27 | 200 | 13.310188954s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:36:27 | 404 |     727.335µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:27 | 404 |      21.482µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:29 | 200 | 12.150293987s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:36:29 | 404 |      174.17µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:29 | 404 |      65.591µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:29 | 200 |  7.223573942s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:36:29 | 404 |      88.184µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:29 | 404 |     727.157µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:31 | 200 |  6.499960956s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:36:31 | 404 |     275.025µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:31 | 404 |     108.711µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:33 | 200 |  2.514418124s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:36:33 | 404 |      93.584µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:33 | 404 |     142.065µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:33 | 404 |      44.635µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:45 | 200 | 11.648033572s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:36:45 | 404 |      208.11µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:45 | 404 |     160.668µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:45 | 404 |      75.259µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:50 | 200 |  4.958414934s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:36:50 | 404 |     134.557µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:50 | 404 |      93.797µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:50 | 404 |      39.421µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:52 | 200 |  2.153206469s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:36:52 | 404 |     770.942µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:52 | 404 |       90.77µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:53 | 200 |  1.159965861s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:36:53 | 404 |     223.034µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:53 | 404 |     136.767µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:53 | 200 |   17.434592ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2024/12/08 - 07:36:53 | 404 |     137.032µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:53 | 404 |      83.871µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:36:53 | 404 |      38.583µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:37:05 | 200 |  11.80692783s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:37:05 | 404 |     658.052µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:37:05 | 404 |     756.244µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:37:07 | 200 | 14.163468452s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:37:07 | 404 |     194.635µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:37:07 | 404 |      78.331µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:37:09 | 200 |  15.71271299s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:37:09 | 404 |     100.494µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:37:09 | 404 |     169.697µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:37:13 | 200 | 19.234274071s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:37:13 | 404 |     217.266µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:37:13 | 404 |     131.463µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:37:19 | 200 | 11.091306023s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:37:19 | 404 |     183.384µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:37:19 | 404 |      173.99µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:37:22 | 200 | 16.887201607s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:37:22 | 404 |     209.148µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:37:22 | 404 |     143.311µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:37:24 | 200 | 15.053904696s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:37:24 | 404 |      86.825µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:37:24 | 404 |     749.433µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:37:25 | 200 | 12.007046245s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:37:25 | 404 |      78.687µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:37:25 | 404 |      61.984µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:37:26 | 200 |  7.471757996s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:37:26 | 404 |     182.408µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:37:26 | 404 |     125.969µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:37:28 | 200 |   5.85645591s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:37:28 | 404 |      810.04µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:37:28 | 404 |     116.392µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:37:30 | 200 |  2.305359866s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:37:30 | 404 |     900.427µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:37:30 | 404 |     198.366µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:37:30 | 404 |      82.044µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:37:42 | 200 | 11.552536409s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:37:42 | 404 |     229.886µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:37:42 | 404 |     182.738µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:37:42 | 404 |       55.29µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:37:44 | 200 |  2.147896999s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:37:44 | 404 |     873.365µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:37:44 | 404 |     171.237µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:37:45 | 200 |  1.173890106s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:37:45 | 404 |      211.43µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:37:45 | 404 |     183.366µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:37:45 | 200 |   16.694998ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2024/12/08 - 07:37:45 | 404 |     134.743µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:37:45 | 404 |      38.745µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:37:45 | 404 |      55.559µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:37:58 | 200 | 12.275812428s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:37:58 | 404 |     233.925µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:37:58 | 404 |     223.991µs |       127.0.0.1 | POST     "/api/show"
time=2024-12-08T07:37:59.025-05:00 level=ERROR source=runner.go:426 msg="failed to decode batch" error="could not find a KV slot for the batch - try reducing the size of the batch or increase the context. code: 1"
[GIN] 2024/12/08 - 07:38:00 | 200 | 14.354078877s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:38:00 | 404 |     761.128µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:00 | 404 |      67.148µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:00 | 200 | 14.465926014s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:38:00 | 404 |     716.146µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:00 | 404 |      17.604µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:05 | 200 | 20.005080603s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:38:05 | 404 |     584.673µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:05 | 404 |     511.467µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:11 | 200 | 10.832357182s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:38:11 | 404 |      74.532µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:11 | 404 |     239.885µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:13 | 200 |   13.0701841s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:38:13 | 404 |       181.6µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:13 | 404 |      18.136µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:16 | 200 | 10.409269963s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:38:16 | 404 |     106.341µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:16 | 404 |     326.509µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:17 | 200 | 19.053404232s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:38:17 | 404 |     104.963µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:17 | 404 |     183.443µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:18 | 200 |  7.462269111s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:38:18 | 404 |      67.479µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:18 | 404 |     181.003µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:19 | 200 |    5.8269839s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:38:19 | 404 |      93.063µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:19 | 404 |      81.181µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:21 | 200 |  2.313795075s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:38:21 | 404 |     757.835µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:21 | 404 |     552.662µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:21 | 404 |      96.867µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:32 | 200 | 11.373440146s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:38:32 | 404 |     110.059µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:32 | 404 |      105.55µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:33 | 404 |      78.346µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:35 | 200 |  2.138476647s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:38:35 | 404 |     241.029µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:35 | 404 |      85.709µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:36 | 200 |  1.193278617s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:38:36 | 404 |     208.751µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:36 | 404 |     133.195µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:36 | 200 |   16.846244ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2024/12/08 - 07:38:36 | 404 |     142.541µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:36 | 404 |      40.293µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:36 | 404 |      33.362µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:48 | 200 | 11.761728742s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:38:48 | 404 |     177.594µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:48 | 404 |     142.663µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:52 | 200 |   15.6825512s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:38:52 | 404 |     720.165µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:52 | 404 |     161.624µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:52 | 200 | 15.733730778s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:38:52 | 404 |      53.507µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:38:52 | 404 |      60.425µs |       127.0.0.1 | POST     "/api/show"
time=2024-12-08T07:38:54.162-05:00 level=ERROR source=runner.go:426 msg="failed to decode batch" error="could not find a KV slot for the batch - try reducing the size of the batch or increase the context. code: 1"
time=2024-12-08T07:38:54.165-05:00 level=ERROR source=runner.go:426 msg="failed to decode batch" error="could not find a KV slot for the batch - try reducing the size of the batch or increase the context. code: 1"
[GIN] 2024/12/08 - 07:46:53 | 500 |          8m4s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:46:53 | 500 |         8m16s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:46:53 | 500 |          8m0s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:46:53 | 500 |          8m0s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:46:53 | 404 |     459.104µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 07:55:13 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 07:55:13 | 404 |      485.27µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 08:03:33 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 08:03:33 | 404 |     417.725µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 08:11:53 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 08:11:53 | 404 |     515.301µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 08:20:13 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 08:20:13 | 404 |    1.777297ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 08:28:33 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 08:28:33 | 404 |     516.628µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 08:36:53 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 08:36:53 | 404 |     437.238µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 08:45:13 | 500 |         8m19s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 08:45:13 | 404 |     355.658µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 08:53:33 | 500 |         8m19s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 08:53:33 | 404 |     401.564µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 09:01:53 | 500 |         8m19s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 09:01:53 | 404 |     479.127µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 09:10:13 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 09:10:13 | 404 |      1.5725ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 09:18:33 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 09:18:33 | 404 |     513.418µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 09:26:54 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 09:26:54 | 404 |     910.726µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 09:35:14 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 09:35:14 | 404 |     376.743µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 09:43:34 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 09:43:34 | 404 |     372.082µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 09:51:54 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 09:51:54 | 404 |    1.213828ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 10:00:14 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 10:00:14 | 404 |     413.507µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 10:08:34 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 10:08:34 | 404 |     377.109µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 10:16:54 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 10:16:54 | 404 |     381.912µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 10:25:14 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 10:25:14 | 404 |     411.828µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 10:33:34 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 10:33:34 | 404 |    1.292482ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 10:41:54 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 10:41:54 | 404 |     364.731µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 10:50:14 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 10:50:14 | 404 |     643.092µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 10:58:35 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 10:58:35 | 404 |    1.459026ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 11:06:55 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 11:06:55 | 404 |     359.845µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 11:15:15 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 11:15:15 | 404 |      477.51µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 11:23:35 | 500 |         8m19s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 11:23:35 | 404 |     431.862µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 11:31:55 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 11:31:55 | 404 |     952.451µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 11:40:15 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 11:40:15 | 404 |    1.160626ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 11:48:35 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 11:48:35 | 404 |     418.238µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 11:56:55 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 11:56:55 | 404 |     469.257µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 12:05:15 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 12:05:15 | 404 |    1.120077ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 12:13:35 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 12:13:35 | 404 |     1.21562ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 12:21:55 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 12:21:55 | 404 |    1.100602ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 12:30:15 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 12:30:15 | 404 |    1.112592ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 12:38:35 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 12:38:35 | 404 |     863.245µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 12:46:55 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 12:46:55 | 404 |    1.521447ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 12:55:16 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 12:55:16 | 404 |    1.565247ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 13:03:36 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 13:03:36 | 404 |     852.824µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 13:11:56 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 13:11:56 | 404 |    1.024279ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 13:20:16 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 13:20:16 | 404 |    2.013809ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 13:28:36 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 13:28:36 | 404 |     981.398µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 13:36:56 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 13:36:56 | 404 |    1.536323ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 13:45:16 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 13:45:16 | 404 |    1.172017ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 13:53:36 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 13:53:36 | 404 |    1.690224ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 14:01:56 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 14:01:56 | 404 |    2.233215ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 14:10:16 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 14:10:16 | 404 |       986.8µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 14:18:36 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 14:18:36 | 404 |     937.361µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 14:26:57 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 14:26:57 | 404 |     840.007µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 14:35:17 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 14:35:17 | 404 |     566.784µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 14:43:37 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 14:43:37 | 404 |     696.157µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 14:51:57 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 14:51:57 | 404 |     937.194µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 15:00:17 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 15:00:17 | 404 |    3.085983ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 15:08:37 | 500 |         8m19s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 15:08:37 | 404 |    2.073631ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 15:16:57 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 15:16:57 | 404 |     733.966µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 15:25:17 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 15:25:17 | 404 |     788.938µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 15:33:37 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 15:33:37 | 404 |    1.063003ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 15:41:57 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 15:41:57 | 404 |    1.201252ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 15:50:17 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 15:50:17 | 404 |     1.09512ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 15:58:37 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 15:58:37 | 404 |    4.159206ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 16:06:57 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 16:06:57 | 404 |    2.523152ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 16:15:18 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 16:15:18 | 404 |     2.07816ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 16:23:38 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 16:23:38 | 404 |    1.172888ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 16:31:58 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 16:31:58 | 404 |     739.125µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 16:40:18 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 16:40:18 | 404 |     125.855µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 16:48:38 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 16:48:38 | 404 |     629.666µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2024/12/08 - 16:56:58 | 500 |         8m20s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2024/12/08 - 16:56:58 | 404 |     165.371µs |       127.0.0.1 | POST     "/api/show"
slurmstepd: error: *** JOB 1030516 ON atl1-1-03-010-10-0 CANCELLED AT 2024-12-08T17:03:42 ***
---------------------------------------
Begin Slurm Epilog: Dec-08-2024 17:03:44
Job ID:        1030516
Array Job ID:  _4294967294
User ID:       lpimentel3
Account:       ece
Job name:      paperqar_multiagent_llama33_2agents_2rounds
Resources:     cpu=4,gres/gpu:h100=1,mem=128G,node=1
Rsrc Used:     cput=1-15:45:08,vmem=0,walltime=09:56:17,mem=42381448K,energy_used=0
Partition:     coe-gpu
Nodes:         atl1-1-03-010-10-0
---------------------------------------
